{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "bernoullipara = {'alpha': [0.01, 0.1, 0.5,1,10]}\n",
    "decisionpara = {'max_depth': [3, 5, 10,15,20,50]}\n",
    "knnpara = {'n_neighbors': [2,4,5,9,15]}\n",
    "logisticpara = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "randompara = {'n_estimators': [100, 200, 500], 'max_depth': [3, 5, 10,15,20,50]}\n",
    "svmpara = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "models = [\n",
    "    (BernoulliNB(),bernoullipara),\n",
    "    (DecisionTreeClassifier(),decisionpara),\n",
    "    (KNeighborsClassifier(),knnpara),\n",
    "    (LogisticRegression(),logisticpara),\n",
    "    (RandomForestClassifier(), randompara),\n",
    "    (SVC(), svmpara)\n",
    "]\n",
    "\n",
    "mnames=['Bernoulli','Decision','Kneighbors','logistic','Random','SVC']\n",
    "table = pd.DataFrame(mnames, columns=['Models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Creditcard_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1      9\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=df1.Class.value_counts(sort= True)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "classes.plot(kind = 'bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Observations')\n",
    "plt.title('Imbalanced')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1    763\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df1['Class']\n",
    "X=df1.drop('Class',axis=1)\n",
    "#We can use over and under sampling to make data balanced. \n",
    "# I used SMOTE to oversample my data as undersampling is not a good choice here (size only 18). which is very less to train our data after making a sample out of it \n",
    "from imblearn.over_sampling import SMOTE\n",
    "smot = SMOTE(random_state = 2)\n",
    "X_new,y_new = smot.fit_resample(X,y)\n",
    "y_new.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1    763\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new=pd.concat([X_new,y_new],axis=1)\n",
    "classify_new=df_new.Class.value_counts(sort= True)\n",
    "classify_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "classify_new.plot(kind = 'bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Observations')\n",
    "plt.title('balanced')\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "1361   389 -1.207761 -1.330960  1.696821  0.748215  1.835482  0.463235   \n",
      "511    377  1.166919  0.027049  0.513875  0.860965 -0.519452 -0.681147   \n",
      "9        9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
      "393    284 -0.810756  0.654499  2.217257  0.104341 -0.286801  0.117833   \n",
      "471    346  1.077079  0.284980  0.007731  1.657073  0.052020  0.446389   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "829    240 -0.677673  0.992140 -0.197271  1.337549  0.399417 -0.276864   \n",
      "530    394  1.293053  0.457969 -1.940450  0.173149  2.609570  3.014117   \n",
      "1363     1  1.178929  0.269444  0.169779  0.444303  0.068837 -0.078502   \n",
      "795    277 -0.476912  0.399679  0.675193 -0.119410  0.852615  0.086466   \n",
      "1370   494 -1.078786  0.333963  1.638110  0.026614  0.957878 -0.787730   \n",
      "\n",
      "            V7        V8        V9  ...       V21       V22       V23  \\\n",
      "1361 -1.190303  0.540326  0.545271  ...  0.211753  0.592465  0.241713   \n",
      "511   0.074992 -0.187776  0.345399  ... -0.202750 -0.441391 -0.025782   \n",
      "9     0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n",
      "393   0.287552 -0.736461  0.699092  ...  0.938194  0.571651 -0.101609   \n",
      "471  -0.407036  0.355704  0.626039  ... -0.174337 -0.174161 -0.153375   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "829  -0.595904  0.533753 -0.956498  ...  0.074639 -0.272713 -0.043013   \n",
      "530  -0.269415  0.754420 -0.221009  ... -0.121126 -0.427753 -0.159336   \n",
      "1363 -0.074467  0.085730 -0.253895  ... -0.224654 -0.635704  0.101863   \n",
      "795   0.288046  0.139842 -0.078547  ... -0.144185 -0.324252  0.025598   \n",
      "1370  0.765609 -0.101326 -0.111330  ...  0.020633  0.193354 -0.258694   \n",
      "\n",
      "           V24       V25       V26       V27       V28     Amount  Class  \n",
      "1361 -1.050852 -0.666459  0.560723  0.011600 -0.017691   1.305054      1  \n",
      "511   0.452607  0.467223  0.262577 -0.023834  0.020521  40.830000      0  \n",
      "9    -0.385050 -0.069733  0.094199  0.246219  0.083076   3.680000      0  \n",
      "393   0.363928 -0.170947 -0.471524  0.058958 -0.079157  30.300000      0  \n",
      "471  -0.466331  0.611001 -0.252871  0.090375  0.054820  10.990000      0  \n",
      "...        ...       ...       ...       ...       ...        ...    ...  \n",
      "829  -0.833690 -0.937696  0.107663  0.241005  0.112587   0.678288      1  \n",
      "530   0.857135  0.850055 -0.311685  0.037536  0.050618   1.000000      0  \n",
      "1363 -0.351685  0.149181  0.125311 -0.006200  0.017215   2.670349      1  \n",
      "795  -1.021802 -1.068921  0.125503  0.056526  0.043753   0.993037      1  \n",
      "1370  0.269573  0.256100 -0.324783 -0.149199 -0.173699   1.000000      1  \n",
      "\n",
      "[384 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "z = 1.96 \n",
    "p=0.5\n",
    "e=0.05\n",
    "simple_size = int((z**2 *p * (1-p))/0.05**2)\n",
    "print(simple_size)\n",
    "simple_Sample = df_new.sample(n=simple_size, random_state=0)\n",
    "print(simple_Sample)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.620000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>264</td>\n",
       "      <td>0.382945</td>\n",
       "      <td>0.369423</td>\n",
       "      <td>0.877133</td>\n",
       "      <td>0.489226</td>\n",
       "      <td>0.162918</td>\n",
       "      <td>-1.002651</td>\n",
       "      <td>0.403228</td>\n",
       "      <td>-0.184363</td>\n",
       "      <td>-0.035655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146214</td>\n",
       "      <td>-0.387687</td>\n",
       "      <td>-0.026119</td>\n",
       "      <td>0.357784</td>\n",
       "      <td>0.285914</td>\n",
       "      <td>-0.124410</td>\n",
       "      <td>-0.055746</td>\n",
       "      <td>-0.031683</td>\n",
       "      <td>2.014954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>555</td>\n",
       "      <td>0.492592</td>\n",
       "      <td>-0.261778</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.974898</td>\n",
       "      <td>-0.052258</td>\n",
       "      <td>-1.067008</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>-0.184951</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118124</td>\n",
       "      <td>-0.597211</td>\n",
       "      <td>0.349717</td>\n",
       "      <td>0.229828</td>\n",
       "      <td>0.231558</td>\n",
       "      <td>0.051743</td>\n",
       "      <td>-0.063206</td>\n",
       "      <td>0.031801</td>\n",
       "      <td>95.161340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>278</td>\n",
       "      <td>-1.208271</td>\n",
       "      <td>1.420871</td>\n",
       "      <td>-1.050221</td>\n",
       "      <td>2.879562</td>\n",
       "      <td>-0.338765</td>\n",
       "      <td>-1.003062</td>\n",
       "      <td>-1.762814</td>\n",
       "      <td>0.980029</td>\n",
       "      <td>-1.977849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283149</td>\n",
       "      <td>-0.225220</td>\n",
       "      <td>-0.286736</td>\n",
       "      <td>0.112252</td>\n",
       "      <td>0.083160</td>\n",
       "      <td>0.161475</td>\n",
       "      <td>0.176042</td>\n",
       "      <td>-0.093498</td>\n",
       "      <td>0.847480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>517</td>\n",
       "      <td>-1.721457</td>\n",
       "      <td>-1.742412</td>\n",
       "      <td>2.278205</td>\n",
       "      <td>0.890897</td>\n",
       "      <td>2.072778</td>\n",
       "      <td>0.205123</td>\n",
       "      <td>-1.332251</td>\n",
       "      <td>0.542429</td>\n",
       "      <td>0.672296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329778</td>\n",
       "      <td>0.957523</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>-0.533750</td>\n",
       "      <td>-0.060457</td>\n",
       "      <td>0.518827</td>\n",
       "      <td>-0.119372</td>\n",
       "      <td>-0.159272</td>\n",
       "      <td>1.369876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>230</td>\n",
       "      <td>-0.135090</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>0.720383</td>\n",
       "      <td>0.129065</td>\n",
       "      <td>0.852819</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>-0.127021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088125</td>\n",
       "      <td>-0.243794</td>\n",
       "      <td>0.065839</td>\n",
       "      <td>-0.999654</td>\n",
       "      <td>-1.018342</td>\n",
       "      <td>-0.034811</td>\n",
       "      <td>0.161734</td>\n",
       "      <td>0.156045</td>\n",
       "      <td>0.992083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1137 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "3        1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "6        4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7        7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8        7 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1524   264  0.382945  0.369423  0.877133  0.489226  0.162918 -1.002651   \n",
       "1525   555  0.492592 -0.261778  0.445946  0.974898 -0.052258 -1.067008   \n",
       "1526   278 -1.208271  1.420871 -1.050221  2.879562 -0.338765 -1.003062   \n",
       "1527   517 -1.721457 -1.742412  2.278205  0.890897  2.072778  0.205123   \n",
       "1528   230 -0.135090  0.519203  0.720383  0.129065  0.852819  0.011468   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "6    -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
       "7     1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
       "8     0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1524  0.403228 -0.184363 -0.035655  ... -0.146214 -0.387687 -0.026119   \n",
       "1525  0.135417 -0.184951 -0.036315  ... -0.118124 -0.597211  0.349717   \n",
       "1526 -1.762814  0.980029 -1.977849  ...  0.283149 -0.225220 -0.286736   \n",
       "1527 -1.332251  0.542429  0.672296  ...  0.329778  0.957523  0.153190   \n",
       "1528  0.417669  0.077835 -0.127021  ... -0.088125 -0.243794  0.065839   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  Class  \n",
       "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.620000      0  \n",
       "3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.500000      0  \n",
       "6    -0.780055  0.750137 -0.257237  0.034507  0.005168    4.990000      0  \n",
       "7    -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.800000      0  \n",
       "8     1.011592  0.373205 -0.384157  0.011747  0.142404   93.200000      0  \n",
       "...        ...       ...       ...       ...       ...         ...    ...  \n",
       "1524  0.357784  0.285914 -0.124410 -0.055746 -0.031683    2.014954      1  \n",
       "1525  0.229828  0.231558  0.051743 -0.063206  0.031801   95.161340      1  \n",
       "1526  0.112252  0.083160  0.161475  0.176042 -0.093498    0.847480      1  \n",
       "1527 -0.533750 -0.060457  0.518827 -0.119372 -0.159272    1.369876      1  \n",
       "1528 -0.999654 -1.018342 -0.034811  0.161734  0.156045    0.992083      1  \n",
       "\n",
       "[1137 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chat gpt was used here as this is a new technique of merging columns with a merge indicator which i was not awar eof\n",
    "#population - sample = test sample\n",
    "merged = pd.merge(df_new, simple_Sample, how='left', indicator=True)\n",
    "\n",
    "remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=simple_Sample['Class']\n",
    "X_train = simple_Sample.drop('Class',axis=1)\n",
    "X_test = remaining.drop('Class',axis=1)\n",
    "y_test = remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200\n",
       "1    184\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    579\n",
       "0    558\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5}\n",
      "{'max_depth': 15}\n",
      "{'n_neighbors': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "{'max_depth': 10, 'n_estimators': 200}\n",
      "{'C': 100, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "simple_accuracy=[]\n",
    "for model, para in models:\n",
    "    grid = GridSearchCV(model, para, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    simple_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>simple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models    simple\n",
       "0   Bernoulli  0.848725\n",
       "1    Decision  0.940193\n",
       "2  Kneighbors  0.803870\n",
       "3    logistic  0.913808\n",
       "4      Random  0.992084\n",
       "5         SVC  0.871592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['simple'] = simple_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "      Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "27      23  1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083   \n",
      "54      37  1.295668  0.341483  0.081505  0.566746 -0.110459 -0.766325   \n",
      "81      52  1.147369  0.059035  0.263632  1.211023 -0.044096  0.301067   \n",
      "108     73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
      "135     84  1.119272 -0.669639  0.803807 -0.651693 -1.395666 -0.800698   \n",
      "162    103 -0.940893  1.074155  1.759398 -0.601446  0.101693 -0.188520   \n",
      "189    124 -1.710935 -1.366799  2.217311  0.404714 -0.114375 -0.075942   \n",
      "216    142  1.288256  0.085828 -1.179482  0.064357  2.195225  3.383363   \n",
      "243    164 -0.433211  1.020835  2.019730  3.003261  0.031308  0.187063   \n",
      "270    190 -0.549414  0.676861  2.151950  1.014523 -0.620012  0.076154   \n",
      "297    211 -0.247827 -0.282682  1.653354 -1.014865 -0.680433  0.886364   \n",
      "324    237  1.260248 -0.020172 -1.164387  0.266251  2.184886  3.607421   \n",
      "351    259 -1.569485 -1.932133  1.249203 -4.434211  1.244282  0.402688   \n",
      "378    275 -0.363519  0.055464  1.857571 -1.085421 -0.981918 -0.473025   \n",
      "405    292  1.252189 -0.126779  0.280285  0.579416 -0.374125 -0.215217   \n",
      "432    312 -0.719402 -0.124184  1.979309  0.503415 -0.732324  0.394883   \n",
      "459    336 -0.895224  0.562106  2.817524 -0.718734  0.223222  0.796156   \n",
      "486    356  1.586093 -1.169091 -1.350477 -2.504580  1.106389  3.135282   \n",
      "513    379 -1.896099 -1.829046  1.181222  3.897846 -1.686469  1.976371   \n",
      "540    406 -0.814054  1.538222  1.115690 -0.051667  0.092334 -1.013398   \n",
      "567    425 -0.367058  1.035851  1.108064 -0.065459  0.366306 -0.322658   \n",
      "594    446 -1.146103  1.350274  0.907209 -0.040682 -0.242920 -1.099859   \n",
      "621    472 -1.100920  1.029588  1.348333 -1.362082 -0.343465 -0.671659   \n",
      "648    491 -0.946412  0.609500  1.201710  0.113074 -0.210132 -0.954009   \n",
      "675    511 -0.259961  0.998646  1.437975  0.038031  0.125405 -0.957775   \n",
      "702    530  1.217261 -0.080646 -0.059293 -0.868862 -0.236628 -0.700159   \n",
      "729    550 -1.274193  1.722263  0.429337  0.105932 -0.732006 -1.107861   \n",
      "756    564 -0.203837  0.532747 -0.339857 -0.730934  2.728163  3.535882   \n",
      "783    104  0.476731  0.448316  0.348983  0.235098  0.547866  0.131111   \n",
      "810    354 -0.949471 -1.586224  1.709660  0.912247  1.671161  0.371486   \n",
      "837     80  1.234853  0.323521  0.259220  0.615185 -0.234200 -0.755976   \n",
      "864    523 -2.015323 -2.284922  2.274126  1.275356  2.320725  0.498588   \n",
      "891    568  0.890948  0.042776  0.550090  0.741227 -0.040379 -0.880426   \n",
      "918    129  1.114134  0.413500  0.227016  0.823540 -0.377418 -1.084317   \n",
      "945    552 -0.598350  0.171470  0.852125 -0.143946  0.434127 -0.586975   \n",
      "972    546 -1.100494  0.119183  0.999625 -0.369735  0.648374 -0.456979   \n",
      "999    485 -2.805425 -3.006210  1.403224  2.026415  1.611444 -0.686045   \n",
      "1026   548 -0.566256 -1.236402  1.516208  0.942219  1.220722 -0.137244   \n",
      "1053   532 -1.918734 -1.699522  2.067300  0.578835  1.980543  0.317496   \n",
      "1080   540  0.452498  0.377063  0.835265  0.503511  0.130017 -1.006153   \n",
      "1107   489 -2.730074 -2.958396  1.502828  1.943434  1.691074 -0.566184   \n",
      "1134   486 -2.752793 -2.442101  1.110430  1.632425  1.261954 -0.892589   \n",
      "1161   490 -1.086550 -0.029356  1.848403  0.324129  1.187370 -0.680074   \n",
      "1188   234 -0.835159  1.241363 -0.861078  2.501588 -0.276772 -0.859935   \n",
      "1215   407 -2.289073  1.926000 -1.553795  3.934084 -0.497287 -1.417754   \n",
      "1242   532 -1.860852 -0.248030  1.177819 -0.380689  0.961773 -0.364218   \n",
      "1269   537 -1.386105 -1.956090  2.059763  1.055300  1.930239  0.280883   \n",
      "1296   128  0.983771  0.396360  0.336777  0.560425 -0.097344 -0.766909   \n",
      "1323   253  0.449158  0.367970  0.833497  0.504708  0.122339 -1.007784   \n",
      "1350   421 -2.272008  1.378063 -1.083730  3.629111 -0.137089 -1.165768   \n",
      "1377   498 -2.566391 -2.854530  1.719197  1.763176  1.864052 -0.305809   \n",
      "1404   570  0.944486  0.332123  0.398932  0.549792 -0.224146 -0.986390   \n",
      "1431   562  0.306653  0.265706  0.586289  0.262990  0.047994 -0.821266   \n",
      "1458   533 -1.653382  0.089054  1.245302 -0.568421  0.925434 -0.355830   \n",
      "1485   518 -2.028291 -2.099865  2.104485  1.394270  2.196555  0.414504   \n",
      "1512   457 -0.768398  0.394690  1.635892  0.220012  0.868521 -0.913385   \n",
      "\n",
      "            V7        V8        V9  ...       V21       V22       V23  \\\n",
      "0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
      "27   -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357 -0.037710   \n",
      "54    0.073155 -0.168304  0.071837  ... -0.323607 -0.929781  0.063809   \n",
      "81   -0.132960  0.227885  0.252191  ... -0.087813 -0.110756 -0.097771   \n",
      "108   0.638387 -0.136762 -0.805505  ... -0.124012 -0.227150 -0.199185   \n",
      "135  -0.601605  0.014390  2.019905  ...  0.163687  0.546516 -0.176836   \n",
      "162   0.455756 -3.460682  0.441525  ...  2.270069 -0.143518  0.153908   \n",
      "189  -0.259943  0.320897 -0.175355  ...  0.390634  0.481111  0.405839   \n",
      "216  -0.448437  0.799347 -0.147006  ...  0.017485 -0.051355 -0.145480   \n",
      "243   0.850856 -0.143932 -0.918043  ... -0.177298 -0.180260  0.007760   \n",
      "270   0.041578  0.342672  0.124723  ...  0.212024  0.850203 -0.185597   \n",
      "297  -0.538201  0.377970 -1.230879  ...  0.545696  1.400962 -0.133992   \n",
      "324  -0.436997  0.848745  0.172271  ... -0.182469 -0.481653 -0.128006   \n",
      "351  -0.649554  0.534756  0.886183  ... -0.074659  0.397405  0.199030   \n",
      "378   0.210565  0.022670 -1.641689  ... -0.408214 -0.907776  0.248037   \n",
      "405  -0.193078  0.011076  0.770448  ... -0.360296 -0.959573 -0.023837   \n",
      "432   0.280296  0.118478 -1.445666  ... -0.170480 -0.125550  0.203485   \n",
      "459   0.464887 -0.002081  0.387537  ... -0.117009  0.221249 -0.380422   \n",
      "486  -1.351529  0.690932 -2.090697  ... -0.200366 -0.325402 -0.071104   \n",
      "513  -0.564107 -2.400465 -0.979202  ... -0.936346 -0.516626 -0.911682   \n",
      "540   0.748851 -0.124814 -0.207407  ... -0.311452 -0.627544 -0.016469   \n",
      "567   0.596340  0.109672 -0.318671  ... -0.262881 -0.671307 -0.022587   \n",
      "594   0.579042  0.045619  0.460784  ... -0.396947 -0.895759  0.099686   \n",
      "621   0.291222  0.379994  0.338839  ... -0.110388 -0.180427 -0.007197   \n",
      "648   0.130325  0.293270 -0.579076  ... -0.036421 -0.266529 -0.038869   \n",
      "675   0.899758 -0.270159 -0.186834  ... -0.316535 -0.697534 -0.035228   \n",
      "702   0.143747 -0.111374  0.790809  ...  0.081582  0.534693 -0.159644   \n",
      "729  -0.021279  0.818796 -0.447669  ... -0.218749 -0.786584  0.106722   \n",
      "756   0.263680  0.919169 -0.194501  ... -0.082087 -0.271636 -0.157778   \n",
      "783   0.161065  0.119884 -0.170779  ... -0.163739 -0.474502  0.133083   \n",
      "810  -1.441357  0.556805  0.559026  ...  0.209025  0.591533  0.233123   \n",
      "837   0.034104 -0.111234 -0.057288  ... -0.267926 -0.770961  0.119559   \n",
      "864  -2.130408  0.816910  0.782267  ...  0.426939  1.137139  0.261711   \n",
      "891  -0.153874 -0.097887  0.120757  ... -0.207012 -0.593719  0.146819   \n",
      "918  -0.016784 -0.139908 -0.075557  ... -0.255829 -0.801203  0.104668   \n",
      "945   0.202559  0.007086  0.020036  ... -0.220382 -0.431256 -0.113640   \n",
      "972   0.231855  0.065897  0.021545  ... -0.202398 -0.325906 -0.178923   \n",
      "999  -0.230521  0.127667  0.009808  ...  0.607075  0.608970  1.129823   \n",
      "1026 -1.139773  0.348738  0.543101  ...  0.110212  0.307920  0.222925   \n",
      "1053 -1.366996  0.586027  0.666645  ...  0.234412  0.762017  0.123004   \n",
      "1080  0.383361 -0.190514 -0.043224  ... -0.156739 -0.414683 -0.014397   \n",
      "1107 -0.406494  0.189520  0.098653  ...  0.589791  0.663871  1.051933   \n",
      "1134  0.312988 -0.021356 -0.205356  ...  0.474265  0.295666  1.011050   \n",
      "1161  0.437353 -0.016973  0.020548  ...  0.118968  0.416426 -0.175845   \n",
      "1188 -1.501026  0.840908 -1.710089  ...  0.204034 -0.289493 -0.226416   \n",
      "1215 -2.480237  1.365758 -2.726128  ...  0.509690 -0.029756 -0.461744   \n",
      "1242  0.274376  0.121103 -0.004122  ... -0.100724 -0.133240 -0.108420   \n",
      "1269 -1.694456  0.600017  0.780719  ...  0.288687  0.815197  0.265744   \n",
      "1296  0.134871 -0.124268 -0.001155  ... -0.251139 -0.729232  0.133345   \n",
      "1323  0.379198 -0.185766 -0.030278  ... -0.156949 -0.421478 -0.014409   \n",
      "1350 -2.482258  1.313802 -2.288890  ...  0.505001  0.123734 -0.366743   \n",
      "1377 -0.788759  0.323882  0.291651  ...  0.552244  0.783131  0.882733   \n",
      "1404  0.112547 -0.173614  0.015397  ... -0.275638 -0.754942  0.086940   \n",
      "1431  0.149759 -0.098910  0.017315  ... -0.252794 -0.621125  0.004017   \n",
      "1458  0.333200  0.109387  0.006058  ... -0.153696 -0.142270 -0.261468   \n",
      "1485 -2.148184  0.842014  0.627110  ...  0.430883  1.085941  0.229961   \n",
      "1512  0.821063 -0.159973 -0.129150  ...  0.040462  0.199882 -0.229726   \n",
      "\n",
      "           V24       V25       V26       V27       V28      Amount  Class  \n",
      "0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.620000      0  \n",
      "27    0.347151  0.559639 -0.280158  0.042335  0.028822   16.000000      0  \n",
      "54   -0.193565  0.287574  0.127881 -0.023731  0.025200    0.990000      0  \n",
      "81   -0.323374  0.633279 -0.305328  0.027394 -0.000580    6.670000      0  \n",
      "108  -0.289757  0.776244 -0.283950  0.056747  0.084706    1.000000      0  \n",
      "135   0.402556  0.563402 -0.534236  0.075047  0.042001   67.300000      0  \n",
      "162   0.700927 -0.413235  1.374031 -0.996161 -0.836301    9.990000      0  \n",
      "189   0.066433  0.156732  1.286201 -0.093975  0.098826  230.000000      0  \n",
      "216   1.007613  0.833293 -0.265485  0.020539  0.015394    4.900000      0  \n",
      "243   0.382658 -0.187193  0.100067  0.204039 -0.018150   65.260000      0  \n",
      "270   0.544990 -0.130609 -0.196374  0.422119  0.203313   20.700000      0  \n",
      "297  -0.810782 -0.348544  0.093031  0.165626  0.130422   70.000000      0  \n",
      "324   1.009959  0.892621 -0.326798  0.025039  0.012735    9.990000      0  \n",
      "351  -1.386013 -0.141955 -0.984011  0.274079 -0.019784   55.450000      0  \n",
      "378   0.492936 -0.524889  0.340449  0.011119  0.060936   70.330000      0  \n",
      "405  -0.462201  0.381732  0.340518 -0.034929  0.007525   23.880000      0  \n",
      "432   0.041484 -0.026663 -0.232425  0.134531  0.133484  140.600000      0  \n",
      "459  -0.245721  0.202958  0.320802 -0.174340 -0.331954    7.720000      0  \n",
      "486   0.996898  0.681093 -0.100190  0.022118  0.009259   15.000000      0  \n",
      "513   0.025106  0.656699  0.200868 -0.088994  0.339814  881.130000      0  \n",
      "540   0.363403 -0.014631  0.076914  0.467478  0.228123    1.980000      0  \n",
      "567  -0.347485 -0.197110  0.125083  0.258796  0.090689    3.590000      0  \n",
      "594   0.275643 -0.045217  0.095849  0.563119  0.307945   17.990000      0  \n",
      "621   0.068877 -0.410540  0.731643  0.084051 -0.057236    0.920000      0  \n",
      "648   0.481943 -0.277006  0.949985 -0.079616 -0.016606   18.450000      0  \n",
      "675   0.322387 -0.198797  0.044630  0.077318 -0.125135    8.930000      0  \n",
      "702   0.296812  0.855793 -0.553063  0.054274  0.001410    1.000000      0  \n",
      "729   0.313637 -0.083626  0.083494  0.109637  0.018247   16.830000      0  \n",
      "756   0.989458  0.228821 -0.545156  0.058120  0.035573   12.900000      0  \n",
      "783  -0.994712 -0.827960  0.093614  0.144951  0.152490    1.602949      1  \n",
      "810  -0.686799 -0.092070  0.618981 -0.086548 -0.109891    1.891804      1  \n",
      "837   0.123329  0.200427  0.104626 -0.018782  0.025748    2.690000      1  \n",
      "864  -0.801366 -0.206831  0.828674 -0.106358 -0.169745    1.428984      1  \n",
      "891   0.208023  0.171541  0.180687 -0.033719  0.008205    1.313639      1  \n",
      "918   0.338668  0.209178  0.097985 -0.012126  0.024018    2.583837      1  \n",
      "945  -0.016530 -0.123395  0.184789 -0.222235 -0.225824    1.110358      1  \n",
      "972  -0.113832 -0.216601  0.209245 -0.276356 -0.295290    1.061758      1  \n",
      "999  -0.422408  0.165846  0.084400 -0.223516 -0.011455  408.569136      1  \n",
      "1026 -0.328757 -0.025438  0.523546 -0.079531 -0.082137    1.407557      1  \n",
      "1053 -0.663558 -0.255468  0.667138 -0.193500 -0.237438    1.343820      1  \n",
      "1080  0.358694  0.283745 -0.107574 -0.052492 -0.026786    1.183168      1  \n",
      "1107 -0.463104  0.129787  0.157107 -0.214257 -0.026397  370.459525      1  \n",
      "1134 -0.281253  0.142811 -0.059430 -0.273350 -0.057663  411.360357      1  \n",
      "1161  0.201802  0.300923 -0.259577 -0.107429 -0.132458    1.073877      1  \n",
      "1188  0.041971  0.096220  0.155943  0.147279 -0.076675    1.133909      1  \n",
      "1215  0.321289  0.050317  0.167273  0.255029 -0.142983    0.016728      1  \n",
      "1242 -0.242754 -0.277432  0.204185 -0.336476 -0.344274   50.471589      1  \n",
      "1269 -0.630759 -0.136261  0.716445 -0.105305 -0.132965    1.460397      1  \n",
      "1296 -0.051513 -0.152419  0.090276  0.035194  0.076628    2.299840      1  \n",
      "1323  0.356390  0.280601 -0.107771 -0.053286 -0.026931    2.066214      1  \n",
      "1350  0.168272  0.010472  0.266001  0.211363 -0.146861    0.193569      1  \n",
      "1377 -0.551508  0.051456  0.315047 -0.194145 -0.058856  287.674277      1  \n",
      "1404  0.282431  0.162979  0.109647 -0.055949 -0.012389    1.259683      1  \n",
      "1431  0.158836  0.044587  0.140712 -0.124695 -0.100626    1.197950      1  \n",
      "1458 -0.172000 -0.258705  0.167349 -0.319828 -0.356463    1.000000      1  \n",
      "1485 -0.752379 -0.195852  0.800247 -0.090307 -0.168589    1.366569      1  \n",
      "1512  0.382022  0.378303 -0.413726 -0.098516 -0.114306    1.123626      1  \n",
      "\n",
      "[57 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "n = len(df1)\n",
    "m = int(math.sqrt(n))\n",
    "print(m)\n",
    "#evry m row \n",
    "system_sample = df_new.iloc[::m]\n",
    "print(system_sample)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df_new, system_sample, how='left', indicator=True)\n",
    "remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "remaining\n",
    "y_train=system_sample['Class']\n",
    "X_train = system_sample.drop('Class',axis=1)\n",
    "X_test = remaining.drop('Class',axis=1)\n",
    "y_test = remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29\n",
       "1    28\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10}\n",
      "BernoulliNB accuracy: 77.33%\n",
      "{'max_depth': 3}\n",
      "DecisionTreeClassifier accuracy: 86.25%\n",
      "{'n_neighbors': 15}\n",
      "KNeighborsClassifier accuracy: 63.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 84.34%\n",
      "{'max_depth': 15, 'n_estimators': 500}\n",
      "RandomForestClassifier accuracy: 91.56%\n",
      "{'C': 1, 'gamma': 0.01}\n",
      "SVC accuracy: 67.73%\n"
     ]
    }
   ],
   "source": [
    "system_accuracy=[]\n",
    "for model, para in models:\n",
    "    grid = GridSearchCV(model, para, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    system_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>simple</th>\n",
       "      <th>system</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models    simple    system\n",
       "0   Bernoulli  0.848725  0.773315\n",
       "1    Decision  0.940193  0.862491\n",
       "2  Kneighbors  0.803870  0.637849\n",
       "3    logistic  0.913808  0.843431\n",
       "4      Random  0.992084  0.915589\n",
       "5         SVC  0.871592  0.677332"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['system'] = system_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "strata = 'Class'\n",
    "# Determine the number of strata based on the unique values of the stratification variable\n",
    "S = df_new[strata].nunique()\n",
    "\n",
    "z = 1.96 \n",
    "e=0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 200, 1: 200}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proportionate stratification:\n",
    "\n",
    "sample_size = {}\n",
    "for i in range(S):\n",
    "    N = len(df_new[df_new[strata] == i])\n",
    "    p = df_new[df_new[strata] == i][strata].sum() / N \n",
    "    n = np.ceil((z**2 * p * (1 - p)) / ((e / S)**2 ))\n",
    "    sample_size[i] = max(int(n), 200)\n",
    "\n",
    "\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_sample = pd.concat([df_new.loc[df_new[strata] == i].sample(n=sample_size[i], random_state=42) for i in range(S)])\n",
    "merged = pd.merge(df_new, stratified_sample, how='left', indicator=True)\n",
    "remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "remaining\n",
    "y_train=stratified_sample['Class']\n",
    "X_train = stratified_sample.drop('Class',axis=1)\n",
    "X_test = remaining.drop('Class',axis=1)\n",
    "y_test = remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200\n",
       "1    200\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01}\n",
      "BernoulliNB accuracy: 86.36%\n",
      "{'max_depth': 20}\n",
      "DecisionTreeClassifier accuracy: 95.90%\n",
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier accuracy: 80.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 90.11%\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "RandomForestClassifier accuracy: 98.84%\n",
      "{'C': 100, 'gamma': 0.001}\n",
      "SVC accuracy: 90.37%\n"
     ]
    }
   ],
   "source": [
    "stratified_accuracy=[]\n",
    "for model, para in models:\n",
    "    grid = GridSearchCV(model, para, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    stratified_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>simple</th>\n",
       "      <th>system</th>\n",
       "      <th>stratified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.959002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.808378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.901070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.988414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.903743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models    simple    system  stratified\n",
       "0   Bernoulli  0.848725  0.773315    0.863636\n",
       "1    Decision  0.940193  0.862491    0.959002\n",
       "2  Kneighbors  0.803870  0.637849    0.808378\n",
       "3    logistic  0.913808  0.843431    0.901070\n",
       "4      Random  0.992084  0.915589    0.988414\n",
       "5         SVC  0.871592  0.677332    0.903743"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['stratified'] = stratified_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1180\n",
       "7     175\n",
       "4      69\n",
       "3      53\n",
       "5      36\n",
       "1      10\n",
       "6       2\n",
       "2       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "dataset= df_new\n",
    "num = 8\n",
    "\n",
    "kmeans = KMeans(n_clusters=num, random_state=42)\n",
    "\n",
    "kmeans.fit(X_new[['Amount']])\n",
    "\n",
    "labels = kmeans.labels_\n",
    "dataset['clusters'] = labels\n",
    "print(\"Cluster sizes:\")\n",
    "size_cluster =pd.Series(labels).value_counts()\n",
    "size_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusters\n",
       "0    0.579661\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.566038\n",
       "4    0.289855\n",
       "5    0.583333\n",
       "6    0.000000\n",
       "7    0.045714\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauds = dataset.groupby('clusters')['Class'].mean()\n",
    "frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#avg size of clusters\n",
    "z=1.96\n",
    "e=0.05\n",
    "c=0\n",
    "for i in range(num):\n",
    "    c=c+size_cluster[i]\n",
    "\n",
    "c = int(c/8) \n",
    "c= round(c/len(df1),2)\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 24, 1: 0, 2: 0, 3: 24, 4: 20, 5: 24, 6: 0, 7: 5}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sizes = {}\n",
    "for i in range(num):\n",
    "    p_i = frauds[i]\n",
    "    n_i = int(np.ceil((z**2 * p_i * (1 - p_i)) / ((e/c)**2)))\n",
    "    sample_sizes[i] = n_i\n",
    "\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>487</td>\n",
       "      <td>-2.747415</td>\n",
       "      <td>-2.428872</td>\n",
       "      <td>1.110836</td>\n",
       "      <td>1.620286</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>-0.889403</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.204143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293080</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>-0.281021</td>\n",
       "      <td>0.140277</td>\n",
       "      <td>-0.057841</td>\n",
       "      <td>-0.273731</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>409.184350</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>403</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>-2.631727</td>\n",
       "      <td>0.661576</td>\n",
       "      <td>-0.707254</td>\n",
       "      <td>-2.261000</td>\n",
       "      <td>0.360969</td>\n",
       "      <td>-1.069656</td>\n",
       "      <td>-0.053544</td>\n",
       "      <td>-0.833647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068132</td>\n",
       "      <td>-0.499373</td>\n",
       "      <td>-0.028743</td>\n",
       "      <td>0.460852</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.092994</td>\n",
       "      <td>411.880000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>353</td>\n",
       "      <td>0.150999</td>\n",
       "      <td>-3.002120</td>\n",
       "      <td>0.824301</td>\n",
       "      <td>0.231721</td>\n",
       "      <td>-2.621415</td>\n",
       "      <td>0.128843</td>\n",
       "      <td>-0.755233</td>\n",
       "      <td>0.095956</td>\n",
       "      <td>0.638219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320297</td>\n",
       "      <td>-0.591253</td>\n",
       "      <td>0.447557</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>-0.131897</td>\n",
       "      <td>-0.047547</td>\n",
       "      <td>0.130266</td>\n",
       "      <td>611.760000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>444</td>\n",
       "      <td>0.548588</td>\n",
       "      <td>-1.106394</td>\n",
       "      <td>-0.650881</td>\n",
       "      <td>0.572799</td>\n",
       "      <td>-0.148004</td>\n",
       "      <td>0.055981</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>-0.133970</td>\n",
       "      <td>-0.204957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279628</td>\n",
       "      <td>-0.568729</td>\n",
       "      <td>-0.779207</td>\n",
       "      <td>0.507367</td>\n",
       "      <td>0.410569</td>\n",
       "      <td>-0.121734</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>399.070000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.000916</td>\n",
       "      <td>-3.085668</td>\n",
       "      <td>1.101613</td>\n",
       "      <td>2.246211</td>\n",
       "      <td>1.351878</td>\n",
       "      <td>-1.061522</td>\n",
       "      <td>0.336726</td>\n",
       "      <td>-0.069583</td>\n",
       "      <td>-0.268357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432372</td>\n",
       "      <td>1.343044</td>\n",
       "      <td>-0.280118</td>\n",
       "      <td>0.282041</td>\n",
       "      <td>-0.151577</td>\n",
       "      <td>-0.249785</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>518.361360</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>312</td>\n",
       "      <td>0.230981</td>\n",
       "      <td>-2.000483</td>\n",
       "      <td>0.555155</td>\n",
       "      <td>0.600646</td>\n",
       "      <td>-1.551024</td>\n",
       "      <td>0.242333</td>\n",
       "      <td>-0.133183</td>\n",
       "      <td>0.105984</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.610378</td>\n",
       "      <td>-0.309129</td>\n",
       "      <td>0.167024</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.922421</td>\n",
       "      <td>-0.113353</td>\n",
       "      <td>0.093969</td>\n",
       "      <td>493.440000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>482</td>\n",
       "      <td>-2.843333</td>\n",
       "      <td>-2.664819</td>\n",
       "      <td>1.103589</td>\n",
       "      <td>1.836774</td>\n",
       "      <td>1.292425</td>\n",
       "      <td>-0.946223</td>\n",
       "      <td>0.316907</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>-0.225783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339204</td>\n",
       "      <td>1.124686</td>\n",
       "      <td>-0.285161</td>\n",
       "      <td>0.185469</td>\n",
       "      <td>-0.086190</td>\n",
       "      <td>-0.266942</td>\n",
       "      <td>-0.028569</td>\n",
       "      <td>447.993824</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.954564</td>\n",
       "      <td>-2.938436</td>\n",
       "      <td>1.095185</td>\n",
       "      <td>2.087824</td>\n",
       "      <td>1.329860</td>\n",
       "      <td>-1.012114</td>\n",
       "      <td>0.321723</td>\n",
       "      <td>-0.053583</td>\n",
       "      <td>-0.250879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392691</td>\n",
       "      <td>1.264292</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>0.237876</td>\n",
       "      <td>-0.119064</td>\n",
       "      <td>-0.259070</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>492.999212</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>156</td>\n",
       "      <td>-3.494861</td>\n",
       "      <td>-2.894450</td>\n",
       "      <td>1.637989</td>\n",
       "      <td>-0.274976</td>\n",
       "      <td>-0.389203</td>\n",
       "      <td>-0.703275</td>\n",
       "      <td>0.444194</td>\n",
       "      <td>0.154266</td>\n",
       "      <td>0.695818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.676254</td>\n",
       "      <td>0.596377</td>\n",
       "      <td>0.114229</td>\n",
       "      <td>0.834915</td>\n",
       "      <td>0.309675</td>\n",
       "      <td>0.632261</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>488</td>\n",
       "      <td>-2.331237</td>\n",
       "      <td>-2.574044</td>\n",
       "      <td>0.959037</td>\n",
       "      <td>2.024008</td>\n",
       "      <td>1.075365</td>\n",
       "      <td>-1.065263</td>\n",
       "      <td>0.287270</td>\n",
       "      <td>-0.091393</td>\n",
       "      <td>-0.223688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227457</td>\n",
       "      <td>1.169242</td>\n",
       "      <td>-0.188325</td>\n",
       "      <td>0.270081</td>\n",
       "      <td>-0.105658</td>\n",
       "      <td>-0.214588</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>441.609384</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>384</td>\n",
       "      <td>0.450790</td>\n",
       "      <td>-2.168533</td>\n",
       "      <td>-0.824039</td>\n",
       "      <td>-0.351021</td>\n",
       "      <td>-0.984033</td>\n",
       "      <td>-0.079024</td>\n",
       "      <td>0.274409</td>\n",
       "      <td>-0.183159</td>\n",
       "      <td>-0.949651</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.057971</td>\n",
       "      <td>-0.559170</td>\n",
       "      <td>-0.469404</td>\n",
       "      <td>0.310075</td>\n",
       "      <td>1.076463</td>\n",
       "      <td>-0.173558</td>\n",
       "      <td>0.065210</td>\n",
       "      <td>510.220000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>478</td>\n",
       "      <td>-2.908935</td>\n",
       "      <td>-2.826193</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>1.984838</td>\n",
       "      <td>1.314504</td>\n",
       "      <td>-0.985084</td>\n",
       "      <td>0.319747</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.240584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370750</td>\n",
       "      <td>1.207023</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>0.216378</td>\n",
       "      <td>-0.105579</td>\n",
       "      <td>-0.262300</td>\n",
       "      <td>-0.007489</td>\n",
       "      <td>474.537114</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>344</td>\n",
       "      <td>-3.495984</td>\n",
       "      <td>-4.088420</td>\n",
       "      <td>2.024845</td>\n",
       "      <td>-0.740363</td>\n",
       "      <td>1.128135</td>\n",
       "      <td>-1.231702</td>\n",
       "      <td>-0.086554</td>\n",
       "      <td>0.157807</td>\n",
       "      <td>1.677621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173006</td>\n",
       "      <td>1.280446</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.760879</td>\n",
       "      <td>-0.828147</td>\n",
       "      <td>-0.298700</td>\n",
       "      <td>-0.061615</td>\n",
       "      <td>456.710000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>473</td>\n",
       "      <td>-3.010884</td>\n",
       "      <td>-3.136585</td>\n",
       "      <td>1.131630</td>\n",
       "      <td>2.252680</td>\n",
       "      <td>1.394316</td>\n",
       "      <td>-1.012875</td>\n",
       "      <td>0.249309</td>\n",
       "      <td>-0.040987</td>\n",
       "      <td>-0.232448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459271</td>\n",
       "      <td>1.342209</td>\n",
       "      <td>-0.311441</td>\n",
       "      <td>0.264170</td>\n",
       "      <td>-0.113851</td>\n",
       "      <td>-0.248761</td>\n",
       "      <td>0.029288</td>\n",
       "      <td>512.483598</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>211</td>\n",
       "      <td>0.263523</td>\n",
       "      <td>-1.812897</td>\n",
       "      <td>-0.311087</td>\n",
       "      <td>0.412930</td>\n",
       "      <td>-0.794605</td>\n",
       "      <td>0.196365</td>\n",
       "      <td>0.416626</td>\n",
       "      <td>-0.062991</td>\n",
       "      <td>0.345392</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042342</td>\n",
       "      <td>-0.480538</td>\n",
       "      <td>-0.414743</td>\n",
       "      <td>0.111706</td>\n",
       "      <td>0.819590</td>\n",
       "      <td>-0.168906</td>\n",
       "      <td>0.078203</td>\n",
       "      <td>526.960000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>152</td>\n",
       "      <td>0.095490</td>\n",
       "      <td>-2.557694</td>\n",
       "      <td>-1.134055</td>\n",
       "      <td>-0.435770</td>\n",
       "      <td>0.728493</td>\n",
       "      <td>4.090476</td>\n",
       "      <td>-0.429329</td>\n",
       "      <td>0.896879</td>\n",
       "      <td>1.004244</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.038538</td>\n",
       "      <td>-0.489645</td>\n",
       "      <td>1.108440</td>\n",
       "      <td>0.072603</td>\n",
       "      <td>0.893183</td>\n",
       "      <td>-0.146017</td>\n",
       "      <td>0.113110</td>\n",
       "      <td>614.870000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>486</td>\n",
       "      <td>-2.752793</td>\n",
       "      <td>-2.442101</td>\n",
       "      <td>1.110430</td>\n",
       "      <td>1.632425</td>\n",
       "      <td>1.261954</td>\n",
       "      <td>-0.892589</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>-0.021356</td>\n",
       "      <td>-0.205356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295666</td>\n",
       "      <td>1.011050</td>\n",
       "      <td>-0.281253</td>\n",
       "      <td>0.142811</td>\n",
       "      <td>-0.059430</td>\n",
       "      <td>-0.273350</td>\n",
       "      <td>-0.057663</td>\n",
       "      <td>411.360357</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>481</td>\n",
       "      <td>-2.864189</td>\n",
       "      <td>-3.043499</td>\n",
       "      <td>1.325544</td>\n",
       "      <td>2.091130</td>\n",
       "      <td>1.549342</td>\n",
       "      <td>-0.779524</td>\n",
       "      <td>-0.093282</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>-0.059481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566154</td>\n",
       "      <td>1.190569</td>\n",
       "      <td>-0.390669</td>\n",
       "      <td>0.193969</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>-0.230736</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>438.290346</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.965549</td>\n",
       "      <td>-2.965457</td>\n",
       "      <td>1.094355</td>\n",
       "      <td>2.112617</td>\n",
       "      <td>1.333557</td>\n",
       "      <td>-1.018622</td>\n",
       "      <td>0.322198</td>\n",
       "      <td>-0.055337</td>\n",
       "      <td>-0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397974</td>\n",
       "      <td>1.278079</td>\n",
       "      <td>-0.290437</td>\n",
       "      <td>0.243052</td>\n",
       "      <td>-0.122311</td>\n",
       "      <td>-0.258293</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>497.443866</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>492</td>\n",
       "      <td>-0.789890</td>\n",
       "      <td>-1.379371</td>\n",
       "      <td>0.171334</td>\n",
       "      <td>-1.636756</td>\n",
       "      <td>-2.807266</td>\n",
       "      <td>0.726236</td>\n",
       "      <td>2.737602</td>\n",
       "      <td>-0.933999</td>\n",
       "      <td>-2.413730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167077</td>\n",
       "      <td>0.540876</td>\n",
       "      <td>0.067496</td>\n",
       "      <td>0.479636</td>\n",
       "      <td>-0.098230</td>\n",
       "      <td>0.053527</td>\n",
       "      <td>-0.408050</td>\n",
       "      <td>632.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>549</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>-1.818313</td>\n",
       "      <td>1.077334</td>\n",
       "      <td>3.350537</td>\n",
       "      <td>-1.292195</td>\n",
       "      <td>1.546080</td>\n",
       "      <td>-0.282520</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.928263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451008</td>\n",
       "      <td>-0.475113</td>\n",
       "      <td>-0.261082</td>\n",
       "      <td>0.181753</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>-0.019035</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>530.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>386</td>\n",
       "      <td>0.071936</td>\n",
       "      <td>-1.601999</td>\n",
       "      <td>-0.606193</td>\n",
       "      <td>1.591623</td>\n",
       "      <td>-0.364290</td>\n",
       "      <td>0.319968</td>\n",
       "      <td>0.856676</td>\n",
       "      <td>-0.052634</td>\n",
       "      <td>-0.188426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110442</td>\n",
       "      <td>-0.669058</td>\n",
       "      <td>-0.262152</td>\n",
       "      <td>0.555656</td>\n",
       "      <td>-0.288130</td>\n",
       "      <td>-0.089500</td>\n",
       "      <td>0.095584</td>\n",
       "      <td>575.330000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>486</td>\n",
       "      <td>-2.447909</td>\n",
       "      <td>-2.669579</td>\n",
       "      <td>0.980236</td>\n",
       "      <td>2.067354</td>\n",
       "      <td>1.121955</td>\n",
       "      <td>-1.065191</td>\n",
       "      <td>0.293544</td>\n",
       "      <td>-0.087528</td>\n",
       "      <td>-0.231430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>1.203103</td>\n",
       "      <td>-0.205602</td>\n",
       "      <td>0.271672</td>\n",
       "      <td>-0.112161</td>\n",
       "      <td>-0.220842</td>\n",
       "      <td>0.035097</td>\n",
       "      <td>455.923503</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>529</td>\n",
       "      <td>-1.693201</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>1.168646</td>\n",
       "      <td>-0.637328</td>\n",
       "      <td>0.918177</td>\n",
       "      <td>-0.278183</td>\n",
       "      <td>0.269765</td>\n",
       "      <td>0.140603</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196792</td>\n",
       "      <td>-0.251539</td>\n",
       "      <td>-0.265688</td>\n",
       "      <td>-0.361437</td>\n",
       "      <td>0.236193</td>\n",
       "      <td>-0.330682</td>\n",
       "      <td>-0.368193</td>\n",
       "      <td>0.999750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>312</td>\n",
       "      <td>-0.646360</td>\n",
       "      <td>0.353084</td>\n",
       "      <td>0.743939</td>\n",
       "      <td>-0.191564</td>\n",
       "      <td>0.861749</td>\n",
       "      <td>0.035664</td>\n",
       "      <td>0.285499</td>\n",
       "      <td>0.139948</td>\n",
       "      <td>-0.064847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306495</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>-0.916464</td>\n",
       "      <td>-0.970357</td>\n",
       "      <td>0.140924</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>-0.013638</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>499</td>\n",
       "      <td>-1.295440</td>\n",
       "      <td>-0.592966</td>\n",
       "      <td>1.989811</td>\n",
       "      <td>0.510601</td>\n",
       "      <td>1.478677</td>\n",
       "      <td>-0.388836</td>\n",
       "      <td>-0.144863</td>\n",
       "      <td>0.167076</td>\n",
       "      <td>0.234979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594452</td>\n",
       "      <td>-0.067589</td>\n",
       "      <td>-0.040201</td>\n",
       "      <td>0.182026</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.111358</td>\n",
       "      <td>-0.141280</td>\n",
       "      <td>1.171263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>150</td>\n",
       "      <td>0.420672</td>\n",
       "      <td>0.492041</td>\n",
       "      <td>0.407986</td>\n",
       "      <td>0.284861</td>\n",
       "      <td>0.471951</td>\n",
       "      <td>-0.136929</td>\n",
       "      <td>0.234743</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>-0.076560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514391</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>-0.863402</td>\n",
       "      <td>-0.917422</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>1.489568</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>480</td>\n",
       "      <td>1.039079</td>\n",
       "      <td>1.158034</td>\n",
       "      <td>-1.555174</td>\n",
       "      <td>1.500270</td>\n",
       "      <td>1.556148</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>0.499065</td>\n",
       "      <td>0.148227</td>\n",
       "      <td>-0.603040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113742</td>\n",
       "      <td>-0.049555</td>\n",
       "      <td>-0.940148</td>\n",
       "      <td>0.525324</td>\n",
       "      <td>-0.188411</td>\n",
       "      <td>0.116878</td>\n",
       "      <td>0.087125</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>188</td>\n",
       "      <td>1.168433</td>\n",
       "      <td>0.319978</td>\n",
       "      <td>0.589693</td>\n",
       "      <td>1.044774</td>\n",
       "      <td>-0.376942</td>\n",
       "      <td>-0.677335</td>\n",
       "      <td>0.054472</td>\n",
       "      <td>-0.087670</td>\n",
       "      <td>-0.304110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>-0.017274</td>\n",
       "      <td>0.532429</td>\n",
       "      <td>0.502070</td>\n",
       "      <td>-0.470804</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>8.090000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>553</td>\n",
       "      <td>-0.457560</td>\n",
       "      <td>0.186130</td>\n",
       "      <td>0.810769</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>-0.623424</td>\n",
       "      <td>0.194345</td>\n",
       "      <td>-0.009404</td>\n",
       "      <td>0.019612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460793</td>\n",
       "      <td>-0.095337</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>0.177932</td>\n",
       "      <td>-0.207061</td>\n",
       "      <td>-0.206347</td>\n",
       "      <td>1.123985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>549</td>\n",
       "      <td>-0.509941</td>\n",
       "      <td>-1.186967</td>\n",
       "      <td>1.478872</td>\n",
       "      <td>0.934451</td>\n",
       "      <td>1.171985</td>\n",
       "      <td>-0.165965</td>\n",
       "      <td>-1.101672</td>\n",
       "      <td>0.331478</td>\n",
       "      <td>0.526779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273075</td>\n",
       "      <td>0.219984</td>\n",
       "      <td>-0.308013</td>\n",
       "      <td>-0.017825</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>-0.077760</td>\n",
       "      <td>-0.078646</td>\n",
       "      <td>1.403928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>477</td>\n",
       "      <td>-0.347240</td>\n",
       "      <td>1.140218</td>\n",
       "      <td>1.289066</td>\n",
       "      <td>0.061070</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>-0.960933</td>\n",
       "      <td>0.756415</td>\n",
       "      <td>-0.091273</td>\n",
       "      <td>-0.439727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667492</td>\n",
       "      <td>-0.005055</td>\n",
       "      <td>0.335569</td>\n",
       "      <td>-0.148201</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.245460</td>\n",
       "      <td>0.099186</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>97</td>\n",
       "      <td>1.273818</td>\n",
       "      <td>0.058009</td>\n",
       "      <td>-1.312428</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>2.244373</td>\n",
       "      <td>3.296211</td>\n",
       "      <td>-0.342625</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>-0.154915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182883</td>\n",
       "      <td>-0.146981</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.842883</td>\n",
       "      <td>-0.314281</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>269</td>\n",
       "      <td>1.320626</td>\n",
       "      <td>-1.174338</td>\n",
       "      <td>0.713431</td>\n",
       "      <td>-2.177941</td>\n",
       "      <td>-1.511234</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>-1.308064</td>\n",
       "      <td>0.298796</td>\n",
       "      <td>0.788814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365280</td>\n",
       "      <td>0.064822</td>\n",
       "      <td>-0.322003</td>\n",
       "      <td>0.152701</td>\n",
       "      <td>-0.003131</td>\n",
       "      <td>0.075103</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>406</td>\n",
       "      <td>-0.814054</td>\n",
       "      <td>1.538222</td>\n",
       "      <td>1.115690</td>\n",
       "      <td>-0.051667</td>\n",
       "      <td>0.092334</td>\n",
       "      <td>-1.013398</td>\n",
       "      <td>0.748851</td>\n",
       "      <td>-0.124814</td>\n",
       "      <td>-0.207407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627544</td>\n",
       "      <td>-0.016469</td>\n",
       "      <td>0.363403</td>\n",
       "      <td>-0.014631</td>\n",
       "      <td>0.076914</td>\n",
       "      <td>0.467478</td>\n",
       "      <td>0.228123</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>376</td>\n",
       "      <td>1.220896</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.545205</td>\n",
       "      <td>0.838269</td>\n",
       "      <td>-0.491896</td>\n",
       "      <td>-0.691202</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>0.348690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426644</td>\n",
       "      <td>-0.001156</td>\n",
       "      <td>0.451173</td>\n",
       "      <td>0.477482</td>\n",
       "      <td>0.265557</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>12.890000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>414</td>\n",
       "      <td>-1.043068</td>\n",
       "      <td>1.044582</td>\n",
       "      <td>0.292186</td>\n",
       "      <td>-2.378536</td>\n",
       "      <td>-0.189793</td>\n",
       "      <td>-0.920526</td>\n",
       "      <td>0.178722</td>\n",
       "      <td>-0.384095</td>\n",
       "      <td>0.237234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316389</td>\n",
       "      <td>-0.004709</td>\n",
       "      <td>0.268738</td>\n",
       "      <td>-0.177186</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>0.290259</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>46</td>\n",
       "      <td>-0.378245</td>\n",
       "      <td>0.732925</td>\n",
       "      <td>-0.120154</td>\n",
       "      <td>0.185755</td>\n",
       "      <td>2.594269</td>\n",
       "      <td>3.797183</td>\n",
       "      <td>0.059088</td>\n",
       "      <td>0.976768</td>\n",
       "      <td>-0.412661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157140</td>\n",
       "      <td>-0.194659</td>\n",
       "      <td>1.013897</td>\n",
       "      <td>0.145503</td>\n",
       "      <td>-0.237620</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>0.202788</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>273</td>\n",
       "      <td>-1.009456</td>\n",
       "      <td>1.186971</td>\n",
       "      <td>-0.483997</td>\n",
       "      <td>1.877550</td>\n",
       "      <td>0.212349</td>\n",
       "      <td>-0.510227</td>\n",
       "      <td>-0.989987</td>\n",
       "      <td>0.707891</td>\n",
       "      <td>-1.324622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224472</td>\n",
       "      <td>-0.128711</td>\n",
       "      <td>-0.599473</td>\n",
       "      <td>-0.738325</td>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.245093</td>\n",
       "      <td>0.060652</td>\n",
       "      <td>0.540609</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>328</td>\n",
       "      <td>-4.236419</td>\n",
       "      <td>-4.459784</td>\n",
       "      <td>1.381813</td>\n",
       "      <td>1.117080</td>\n",
       "      <td>6.044486</td>\n",
       "      <td>-3.498447</td>\n",
       "      <td>-2.740892</td>\n",
       "      <td>0.372155</td>\n",
       "      <td>-0.214338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432517</td>\n",
       "      <td>0.612507</td>\n",
       "      <td>-1.016362</td>\n",
       "      <td>0.630373</td>\n",
       "      <td>-0.498141</td>\n",
       "      <td>-0.094774</td>\n",
       "      <td>0.208038</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>545</td>\n",
       "      <td>-1.030747</td>\n",
       "      <td>0.894747</td>\n",
       "      <td>2.375620</td>\n",
       "      <td>0.218372</td>\n",
       "      <td>-0.862126</td>\n",
       "      <td>-0.180483</td>\n",
       "      <td>-0.260299</td>\n",
       "      <td>0.592381</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020326</td>\n",
       "      <td>-0.216072</td>\n",
       "      <td>0.435670</td>\n",
       "      <td>0.192864</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>-0.066266</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>115</td>\n",
       "      <td>1.253704</td>\n",
       "      <td>0.348673</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.688416</td>\n",
       "      <td>-0.363193</td>\n",
       "      <td>-1.051306</td>\n",
       "      <td>0.083605</td>\n",
       "      <td>-0.197312</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828960</td>\n",
       "      <td>0.127569</td>\n",
       "      <td>0.326397</td>\n",
       "      <td>0.215008</td>\n",
       "      <td>0.095302</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>347</td>\n",
       "      <td>-0.812644</td>\n",
       "      <td>0.307358</td>\n",
       "      <td>0.811401</td>\n",
       "      <td>-0.262371</td>\n",
       "      <td>0.870712</td>\n",
       "      <td>-0.014188</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.140052</td>\n",
       "      <td>-0.051402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289069</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.813092</td>\n",
       "      <td>-0.873634</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>-0.050355</td>\n",
       "      <td>-0.069957</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>135</td>\n",
       "      <td>1.101762</td>\n",
       "      <td>-0.119018</td>\n",
       "      <td>1.139150</td>\n",
       "      <td>1.844961</td>\n",
       "      <td>-0.817909</td>\n",
       "      <td>0.247770</td>\n",
       "      <td>-0.684181</td>\n",
       "      <td>0.283096</td>\n",
       "      <td>0.286763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371305</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>0.113901</td>\n",
       "      <td>0.152014</td>\n",
       "      <td>1.126049</td>\n",
       "      <td>-0.038695</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>73</td>\n",
       "      <td>1.162281</td>\n",
       "      <td>1.248178</td>\n",
       "      <td>-1.581317</td>\n",
       "      <td>1.475024</td>\n",
       "      <td>1.138357</td>\n",
       "      <td>-1.020373</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>-0.136762</td>\n",
       "      <td>-0.805505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227150</td>\n",
       "      <td>-0.199185</td>\n",
       "      <td>-0.289757</td>\n",
       "      <td>0.776244</td>\n",
       "      <td>-0.283950</td>\n",
       "      <td>0.056747</td>\n",
       "      <td>0.084706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>519</td>\n",
       "      <td>0.764614</td>\n",
       "      <td>1.706191</td>\n",
       "      <td>-1.755823</td>\n",
       "      <td>1.557386</td>\n",
       "      <td>1.101083</td>\n",
       "      <td>-1.529455</td>\n",
       "      <td>0.917702</td>\n",
       "      <td>-0.190132</td>\n",
       "      <td>-0.748935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>-0.459204</td>\n",
       "      <td>-0.421570</td>\n",
       "      <td>0.260012</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>365</td>\n",
       "      <td>-0.456586</td>\n",
       "      <td>0.541708</td>\n",
       "      <td>1.331297</td>\n",
       "      <td>0.559447</td>\n",
       "      <td>0.184045</td>\n",
       "      <td>0.612792</td>\n",
       "      <td>0.632759</td>\n",
       "      <td>0.152253</td>\n",
       "      <td>-0.202196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316126</td>\n",
       "      <td>-0.072149</td>\n",
       "      <td>-0.749725</td>\n",
       "      <td>-0.039644</td>\n",
       "      <td>0.472098</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.099985</td>\n",
       "      <td>74.220000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>60</td>\n",
       "      <td>1.069141</td>\n",
       "      <td>0.043786</td>\n",
       "      <td>0.309867</td>\n",
       "      <td>1.316694</td>\n",
       "      <td>-0.462394</td>\n",
       "      <td>-0.857888</td>\n",
       "      <td>0.207904</td>\n",
       "      <td>-0.191270</td>\n",
       "      <td>0.056398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164781</td>\n",
       "      <td>-0.177564</td>\n",
       "      <td>0.391891</td>\n",
       "      <td>0.624682</td>\n",
       "      <td>-0.288376</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>524</td>\n",
       "      <td>-0.795322</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>2.210568</td>\n",
       "      <td>0.268145</td>\n",
       "      <td>-0.506875</td>\n",
       "      <td>-0.303840</td>\n",
       "      <td>-0.180829</td>\n",
       "      <td>0.085540</td>\n",
       "      <td>0.248346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869922</td>\n",
       "      <td>-0.118656</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>-0.504294</td>\n",
       "      <td>1.259841</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>0.128346</td>\n",
       "      <td>37.890000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>77</td>\n",
       "      <td>-1.185954</td>\n",
       "      <td>-0.218808</td>\n",
       "      <td>1.101005</td>\n",
       "      <td>-1.406997</td>\n",
       "      <td>1.261783</td>\n",
       "      <td>-0.337709</td>\n",
       "      <td>1.191747</td>\n",
       "      <td>-0.427383</td>\n",
       "      <td>0.276679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418817</td>\n",
       "      <td>0.521401</td>\n",
       "      <td>-0.922427</td>\n",
       "      <td>-0.139840</td>\n",
       "      <td>0.709787</td>\n",
       "      <td>-0.076249</td>\n",
       "      <td>-0.383942</td>\n",
       "      <td>49.950000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>53</td>\n",
       "      <td>-1.198968</td>\n",
       "      <td>-1.474100</td>\n",
       "      <td>1.840326</td>\n",
       "      <td>-4.515824</td>\n",
       "      <td>0.327567</td>\n",
       "      <td>-0.174469</td>\n",
       "      <td>0.959726</td>\n",
       "      <td>-1.026456</td>\n",
       "      <td>1.700435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334614</td>\n",
       "      <td>-0.364541</td>\n",
       "      <td>-0.310186</td>\n",
       "      <td>-0.302599</td>\n",
       "      <td>-1.243924</td>\n",
       "      <td>-1.123457</td>\n",
       "      <td>-0.734351</td>\n",
       "      <td>89.170000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "877    487 -2.747415 -2.428872  1.110836  1.620286  1.260144 -0.889403   \n",
       "535    403  0.699599 -2.631727  0.661576 -0.707254 -2.261000  0.360969   \n",
       "478    353  0.150999 -3.002120  0.824301  0.231721 -2.621415  0.128843   \n",
       "593    444  0.548588 -1.106394 -0.650881  0.572799 -0.148004  0.055981   \n",
       "990    472 -3.000916 -3.085668  1.101613  2.246211  1.351878 -1.061522   \n",
       "433    312  0.230981 -2.000483  0.555155  0.600646 -1.551024  0.242333   \n",
       "851    482 -2.843333 -2.664819  1.103589  1.836774  1.292425 -0.946223   \n",
       "1328   476 -2.954564 -2.938436  1.095185  2.087824  1.329860 -1.012114   \n",
       "235    156 -3.494861 -2.894450  1.637989 -0.274976 -0.389203 -0.703275   \n",
       "1435   488 -2.331237 -2.574044  0.959037  2.024008  1.075365 -1.065263   \n",
       "623    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "519    384  0.450790 -2.168533 -0.824039 -0.351021 -0.984033 -0.079024   \n",
       "1266   478 -2.908935 -2.826193  1.098633  1.984838  1.314504 -0.985084   \n",
       "468    344 -3.495984 -4.088420  2.024845 -0.740363  1.128135 -1.231702   \n",
       "1496   473 -3.010884 -3.136585  1.131630  2.252680  1.394316 -1.012875   \n",
       "296    211  0.263523 -1.812897 -0.311087  0.412930 -0.794605  0.196365   \n",
       "227    152  0.095490 -2.557694 -1.134055 -0.435770  0.728493  4.090476   \n",
       "1134   486 -2.752793 -2.442101  1.110430  1.632425  1.261954 -0.892589   \n",
       "861    481 -2.864189 -3.043499  1.325544  2.091130  1.549342 -0.779524   \n",
       "1479   476 -2.965549 -2.965457  1.094355  2.112617  1.333557 -1.018622   \n",
       "651    492 -0.789890 -1.379371  0.171334 -1.636756 -2.807266  0.726236   \n",
       "727    549  0.033854 -1.818313  1.077334  3.350537 -1.292195  1.546080   \n",
       "521    386  0.071936 -1.601999 -0.606193  1.591623 -0.364290  0.319968   \n",
       "801    486 -2.447909 -2.669579  0.980236  2.067354  1.121955 -1.065191   \n",
       "874    529 -1.693201  0.065219  1.168646 -0.637328  0.918177 -0.278183   \n",
       "1009   312 -0.646360  0.353084  0.743939 -0.191564  0.861749  0.035664   \n",
       "1237   499 -1.295440 -0.592966  1.989811  0.510601  1.478677 -0.388836   \n",
       "1502   150  0.420672  0.492041  0.407986  0.284861  0.471951 -0.136929   \n",
       "631    480  1.039079  1.158034 -1.555174  1.500270  1.556148  0.036028   \n",
       "265    188  1.168433  0.319978  0.589693  1.044774 -0.376942 -0.677335   \n",
       "929    553 -0.457560  0.186130  0.810769 -0.080639  0.374057 -0.623424   \n",
       "1041   549 -0.509941 -1.186967  1.478872  0.934451  1.171985 -0.165965   \n",
       "626    477 -0.347240  1.140218  1.289066  0.061070  0.032514 -0.960933   \n",
       "155     97  1.273818  0.058009 -1.312428 -0.048969  2.244373  3.296211   \n",
       "368    269  1.320626 -1.174338  0.713431 -2.177941 -1.511234  0.059314   \n",
       "244    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "540    406 -0.814054  1.538222  1.115690 -0.051667  0.092334 -1.013398   \n",
       "508    376  1.220896  0.116243  0.545205  0.838269 -0.491896 -0.691202   \n",
       "556    414 -1.043068  1.044582  0.292186 -2.378536 -0.189793 -0.920526   \n",
       "71      46 -0.378245  0.732925 -0.120154  0.185755  2.594269  3.797183   \n",
       "1160   273 -1.009456  1.186971 -0.483997  1.877550  0.212349 -0.510227   \n",
       "450    328 -4.236419 -4.459784  1.381813  1.117080  6.044486 -3.498447   \n",
       "721    545 -1.030747  0.894747  2.375620  0.218372 -0.862126 -0.180483   \n",
       "1360   115  1.253704  0.348673  0.299879  0.688416 -0.363193 -1.051306   \n",
       "813    347 -0.812644  0.307358  0.811401 -0.262371  0.870712 -0.014188   \n",
       "202    135  1.101762 -0.119018  1.139150  1.844961 -0.817909  0.247770   \n",
       "108     73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
       "688    519  0.764614  1.706191 -1.755823  1.557386  1.101083 -1.529455   \n",
       "494    365 -0.456586  0.541708  1.331297  0.559447  0.184045  0.612792   \n",
       "91      60  1.069141  0.043786  0.309867  1.316694 -0.462394 -0.857888   \n",
       "693    524 -0.795322  0.081238  2.210568  0.268145 -0.506875 -0.303840   \n",
       "120     77 -1.185954 -0.218808  1.101005 -1.406997  1.261783 -0.337709   \n",
       "83      53 -1.198968 -1.474100  1.840326 -4.515824  0.327567 -0.174469   \n",
       "\n",
       "            V7        V8        V9  ...       V22       V23       V24  \\\n",
       "877   0.312755 -0.020497 -0.204143  ...  0.293080  1.004300 -0.281021   \n",
       "535  -1.069656 -0.053544 -0.833647  ...  0.068132 -0.499373 -0.028743   \n",
       "478  -0.755233  0.095956  0.638219  ...  0.320297 -0.591253  0.447557   \n",
       "593   0.551553 -0.133970 -0.204957  ... -0.279628 -0.568729 -0.779207   \n",
       "990   0.336726 -0.069583 -0.268357  ...  0.432372  1.343044 -0.280118   \n",
       "433  -0.133183  0.105984  1.248546  ... -0.610378 -0.309129  0.167024   \n",
       "851   0.316907 -0.035817 -0.225783  ...  0.339204  1.124686 -0.285161   \n",
       "1328  0.321723 -0.053583 -0.250879  ...  0.392691  1.264292 -0.289963   \n",
       "235   0.444194  0.154266  0.695818  ... -0.063521  0.676254  0.596377   \n",
       "1435  0.287270 -0.091393 -0.223688  ...  0.227457  1.169242 -0.188325   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.435477  1.375966 -0.293803   \n",
       "519   0.274409 -0.183159 -0.949651  ... -1.057971 -0.559170 -0.469404   \n",
       "1266  0.319747 -0.046295 -0.240584  ...  0.370750  1.207023 -0.287993   \n",
       "468  -0.086554  0.157807  1.677621  ... -0.173006  1.280446  0.012697   \n",
       "1496  0.249309 -0.040987 -0.232448  ...  0.459271  1.342209 -0.311441   \n",
       "296   0.416626 -0.062991  0.345392  ... -1.042342 -0.480538 -0.414743   \n",
       "227  -0.429329  0.896879  1.004244  ... -1.038538 -0.489645  1.108440   \n",
       "1134  0.312988 -0.021356 -0.205356  ...  0.295666  1.011050 -0.281253   \n",
       "861  -0.093282  0.079430 -0.059481  ...  0.566154  1.190569 -0.390669   \n",
       "1479  0.322198 -0.055337 -0.253357  ...  0.397974  1.278079 -0.290437   \n",
       "651   2.737602 -0.933999 -2.413730  ...  0.167077  0.540876  0.067496   \n",
       "727  -0.282520  0.402055  0.928263  ... -0.451008 -0.475113 -0.261082   \n",
       "521   0.856676 -0.052634 -0.188426  ... -0.110442 -0.669058 -0.262152   \n",
       "801   0.293544 -0.087528 -0.231430  ...  0.261529  1.203103 -0.205602   \n",
       "874   0.269765  0.140603  0.019795  ... -0.196792 -0.251539 -0.265688   \n",
       "1009  0.285499  0.139948 -0.064847  ... -0.306495 -0.013011 -0.916464   \n",
       "1237 -0.144863  0.167076  0.234979  ...  0.594452 -0.067589 -0.040201   \n",
       "1502  0.234743  0.038898 -0.076560  ... -0.514391  0.144274 -0.863402   \n",
       "631   0.499065  0.148227 -0.603040  ...  0.113742 -0.049555 -0.940148   \n",
       "265   0.054472 -0.087670 -0.304110  ...  0.066970 -0.017274  0.532429   \n",
       "929   0.194345 -0.009404  0.019612  ... -0.460793 -0.095337  0.010752   \n",
       "1041 -1.101672  0.331478  0.526779  ...  0.273075  0.219984 -0.308013   \n",
       "626   0.756415 -0.091273 -0.439727  ... -0.667492 -0.005055  0.335569   \n",
       "155  -0.342625  0.760870 -0.154915  ... -0.182883 -0.146981  1.004700   \n",
       "368  -1.308064  0.298796  0.788814  ... -0.365280  0.064822 -0.322003   \n",
       "244   0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
       "540   0.748851 -0.124814 -0.207407  ... -0.627544 -0.016469  0.363403   \n",
       "508   0.037916 -0.180524  0.348690  ... -0.426644 -0.001156  0.451173   \n",
       "556   0.178722 -0.384095  0.237234  ...  0.316389 -0.004709  0.268738   \n",
       "71    0.059088  0.976768 -0.412661  ... -0.157140 -0.194659  1.013897   \n",
       "1160 -0.989987  0.707891 -1.324622  ... -0.224472 -0.128711 -0.599473   \n",
       "450  -2.740892  0.372155 -0.214338  ... -0.432517  0.612507 -1.016362   \n",
       "721  -0.260299  0.592381 -0.017765  ...  0.020326 -0.216072  0.435670   \n",
       "1360  0.083605 -0.197312  0.029580  ... -0.828960  0.127569  0.326397   \n",
       "813   0.283000  0.140052 -0.051402  ... -0.289069 -0.050900 -0.813092   \n",
       "202  -0.684181  0.283096  0.286763  ...  0.371305 -0.016047  0.113901   \n",
       "108   0.638387 -0.136762 -0.805505  ... -0.227150 -0.199185 -0.289757   \n",
       "688   0.917702 -0.190132 -0.748935  ...  0.022452  0.056012  0.008466   \n",
       "494   0.632759  0.152253 -0.202196  ... -0.316126 -0.072149 -0.749725   \n",
       "91    0.207904 -0.191270  0.056398  ...  0.164781 -0.177564  0.391891   \n",
       "693  -0.180829  0.085540  0.248346  ...  0.869922 -0.118656  0.744594   \n",
       "120   1.191747 -0.427383  0.276679  ... -0.418817  0.521401 -0.922427   \n",
       "83    0.959726 -1.026456  1.700435  ...  0.334614 -0.364541 -0.310186   \n",
       "\n",
       "           V25       V26       V27       V28      Amount  Class  clusters  \n",
       "877   0.140277 -0.057841 -0.273731 -0.059391  409.184350      1         5  \n",
       "535   0.460852 -0.003403  0.020449  0.092994  411.880000      0         5  \n",
       "478   0.164080 -0.131897 -0.047547  0.130266  611.760000      0         5  \n",
       "593   0.507367  0.410569 -0.121734  0.056038  399.070000      0         5  \n",
       "990   0.282041 -0.151577 -0.249785  0.032510  518.361360      1         5  \n",
       "433  -0.137728  0.922421 -0.113353  0.093969  493.440000      0         5  \n",
       "851   0.185469 -0.086190 -0.266942 -0.028569  447.993824      1         5  \n",
       "1328  0.237876 -0.119064 -0.259070  0.007173  492.999212      1         5  \n",
       "235   0.114229  0.834915  0.309675  0.632261  500.000000      0         5  \n",
       "1435  0.270081 -0.105658 -0.214588  0.034966  441.609384      1         5  \n",
       "623   0.279798 -0.145362 -0.252773  0.035764  529.000000      1         5  \n",
       "519   0.310075  1.076463 -0.173558  0.065210  510.220000      0         5  \n",
       "1266  0.216378 -0.105579 -0.262300 -0.007489  474.537114      1         5  \n",
       "468   0.760879 -0.828147 -0.298700 -0.061615  456.710000      0         5  \n",
       "1496  0.264170 -0.113851 -0.248761  0.029288  512.483598      1         5  \n",
       "296   0.111706  0.819590 -0.168906  0.078203  526.960000      0         5  \n",
       "227   0.072603  0.893183 -0.146017  0.113110  614.870000      0         5  \n",
       "1134  0.142811 -0.059430 -0.273350 -0.057663  411.360357      1         5  \n",
       "861   0.193969  0.027697 -0.230736  0.000198  438.290346      1         5  \n",
       "1479  0.243052 -0.122311 -0.258293  0.010703  497.443866      1         5  \n",
       "651   0.479636 -0.098230  0.053527 -0.408050  632.400000      0         5  \n",
       "727   0.181753  0.025919 -0.019035  0.117564  530.850000      0         5  \n",
       "521   0.555656 -0.288130 -0.089500  0.095584  575.330000      0         5  \n",
       "801   0.271672 -0.112161 -0.220842  0.035097  455.923503      1         5  \n",
       "874  -0.361437  0.236193 -0.330682 -0.368193    0.999750      1         0  \n",
       "1009 -0.970357  0.140924  0.002582 -0.013638    0.993973      1         0  \n",
       "1237  0.182026 -0.003475 -0.111358 -0.141280    1.171263      1         0  \n",
       "1502 -0.917422  0.081081  0.156784  0.171610    1.489568      1         0  \n",
       "631   0.525324 -0.188411  0.116878  0.087125    0.750000      0         0  \n",
       "265   0.502070 -0.470804  0.021893  0.019337    8.090000      0         0  \n",
       "929  -0.097262  0.177932 -0.207061 -0.206347    1.123985      1         0  \n",
       "1041 -0.017825  0.510296 -0.077760 -0.078646    1.403928      1         0  \n",
       "626  -0.148201  0.071064  0.245460  0.099186    9.510000      0         0  \n",
       "155   0.842883 -0.314281  0.013744  0.015907   17.800000      0         0  \n",
       "368   0.152701 -0.003131  0.075103  0.010941    2.600000      0         0  \n",
       "244  -1.389079  0.075412  0.231750  0.230171    0.990000      1         0  \n",
       "540  -0.014631  0.076914  0.467478  0.228123    1.980000      0         0  \n",
       "508   0.477482  0.265557 -0.018555  0.015656   12.890000      0         0  \n",
       "556  -0.177186 -0.126936  0.290259  0.127530    1.000000      0         0  \n",
       "71    0.145503 -0.237620  0.411372  0.202788   11.450000      0         0  \n",
       "1160 -0.738325  0.121907  0.245093  0.060652    0.540609      1         0  \n",
       "450   0.630373 -0.498141 -0.094774  0.208038    2.500000      0         0  \n",
       "721   0.192864  0.351030 -0.066266  0.005583    3.980000      0         0  \n",
       "1360  0.215008  0.095302 -0.023079  0.030582    2.690000      1         0  \n",
       "813  -0.873634  0.156057 -0.050355 -0.069957    0.994890      1         0  \n",
       "202   0.152014  1.126049 -0.038695  0.012339   22.370000      0         0  \n",
       "108   0.776244 -0.283950  0.056747  0.084706    1.000000      0         0  \n",
       "688  -0.459204 -0.421570  0.260012  0.006571    0.890000      0         0  \n",
       "494  -0.039644  0.472098  0.075612  0.099985   74.220000      0         7  \n",
       "91    0.624682 -0.288376 -0.001312  0.035812   89.400000      0         7  \n",
       "693  -0.504294  1.259841  0.014692  0.128346   37.890000      0         7  \n",
       "120  -0.139840  0.709787 -0.076249 -0.383942   49.950000      0         7  \n",
       "83   -0.302599 -1.243924 -1.123457 -0.734351   89.170000      0         7  \n",
       "\n",
       "[53 rows x 32 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "selected = np.random.choice(range(num), size=int(num * 0.5), replace=False)\n",
    "\n",
    "cluster_Sample = pd.DataFrame()\n",
    "for cluster in selected:\n",
    "    sample = dataset[dataset['clusters'] == cluster].sample(sample_sizes[cluster], replace=False)\n",
    "    cluster_Sample = pd.concat([cluster_Sample, sample])\n",
    "\n",
    "cluster_Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df_new, cluster_Sample, how='left', indicator=True)\n",
    "df_remaining = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "df_remaining\n",
    "y_train=cluster_Sample['Class']\n",
    "X_train = cluster_Sample.drop('Class', axis=1)\n",
    "X_test = df_remaining.drop('Class',axis=1)\n",
    "y_test = df_remaining['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>487</td>\n",
       "      <td>-2.747415</td>\n",
       "      <td>-2.428872</td>\n",
       "      <td>1.110836</td>\n",
       "      <td>1.620286</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>-0.889403</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.204143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470798</td>\n",
       "      <td>0.293080</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>-0.281021</td>\n",
       "      <td>0.140277</td>\n",
       "      <td>-0.057841</td>\n",
       "      <td>-0.273731</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>409.184350</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>403</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>-2.631727</td>\n",
       "      <td>0.661576</td>\n",
       "      <td>-0.707254</td>\n",
       "      <td>-2.261000</td>\n",
       "      <td>0.360969</td>\n",
       "      <td>-1.069656</td>\n",
       "      <td>-0.053544</td>\n",
       "      <td>-0.833647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0.068132</td>\n",
       "      <td>-0.499373</td>\n",
       "      <td>-0.028743</td>\n",
       "      <td>0.460852</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.092994</td>\n",
       "      <td>411.880000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>353</td>\n",
       "      <td>0.150999</td>\n",
       "      <td>-3.002120</td>\n",
       "      <td>0.824301</td>\n",
       "      <td>0.231721</td>\n",
       "      <td>-2.621415</td>\n",
       "      <td>0.128843</td>\n",
       "      <td>-0.755233</td>\n",
       "      <td>0.095956</td>\n",
       "      <td>0.638219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521719</td>\n",
       "      <td>0.320297</td>\n",
       "      <td>-0.591253</td>\n",
       "      <td>0.447557</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>-0.131897</td>\n",
       "      <td>-0.047547</td>\n",
       "      <td>0.130266</td>\n",
       "      <td>611.760000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>444</td>\n",
       "      <td>0.548588</td>\n",
       "      <td>-1.106394</td>\n",
       "      <td>-0.650881</td>\n",
       "      <td>0.572799</td>\n",
       "      <td>-0.148004</td>\n",
       "      <td>0.055981</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>-0.133970</td>\n",
       "      <td>-0.204957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212745</td>\n",
       "      <td>-0.279628</td>\n",
       "      <td>-0.568729</td>\n",
       "      <td>-0.779207</td>\n",
       "      <td>0.507367</td>\n",
       "      <td>0.410569</td>\n",
       "      <td>-0.121734</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>399.070000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.000916</td>\n",
       "      <td>-3.085668</td>\n",
       "      <td>1.101613</td>\n",
       "      <td>2.246211</td>\n",
       "      <td>1.351878</td>\n",
       "      <td>-1.061522</td>\n",
       "      <td>0.336726</td>\n",
       "      <td>-0.069583</td>\n",
       "      <td>-0.268357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.432372</td>\n",
       "      <td>1.343044</td>\n",
       "      <td>-0.280118</td>\n",
       "      <td>0.282041</td>\n",
       "      <td>-0.151577</td>\n",
       "      <td>-0.249785</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>518.361360</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>312</td>\n",
       "      <td>0.230981</td>\n",
       "      <td>-2.000483</td>\n",
       "      <td>0.555155</td>\n",
       "      <td>0.600646</td>\n",
       "      <td>-1.551024</td>\n",
       "      <td>0.242333</td>\n",
       "      <td>-0.133183</td>\n",
       "      <td>0.105984</td>\n",
       "      <td>1.248546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079669</td>\n",
       "      <td>-0.610378</td>\n",
       "      <td>-0.309129</td>\n",
       "      <td>0.167024</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.922421</td>\n",
       "      <td>-0.113353</td>\n",
       "      <td>0.093969</td>\n",
       "      <td>493.440000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>482</td>\n",
       "      <td>-2.843333</td>\n",
       "      <td>-2.664819</td>\n",
       "      <td>1.103589</td>\n",
       "      <td>1.836774</td>\n",
       "      <td>1.292425</td>\n",
       "      <td>-0.946223</td>\n",
       "      <td>0.316907</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>-0.225783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532632</td>\n",
       "      <td>0.339204</td>\n",
       "      <td>1.124686</td>\n",
       "      <td>-0.285161</td>\n",
       "      <td>0.185469</td>\n",
       "      <td>-0.086190</td>\n",
       "      <td>-0.266942</td>\n",
       "      <td>-0.028569</td>\n",
       "      <td>447.993824</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.954564</td>\n",
       "      <td>-2.938436</td>\n",
       "      <td>1.095185</td>\n",
       "      <td>2.087824</td>\n",
       "      <td>1.329860</td>\n",
       "      <td>-1.012114</td>\n",
       "      <td>0.321723</td>\n",
       "      <td>-0.053583</td>\n",
       "      <td>-0.250879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604337</td>\n",
       "      <td>0.392691</td>\n",
       "      <td>1.264292</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>0.237876</td>\n",
       "      <td>-0.119064</td>\n",
       "      <td>-0.259070</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>492.999212</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>156</td>\n",
       "      <td>-3.494861</td>\n",
       "      <td>-2.894450</td>\n",
       "      <td>1.637989</td>\n",
       "      <td>-0.274976</td>\n",
       "      <td>-0.389203</td>\n",
       "      <td>-0.703275</td>\n",
       "      <td>0.444194</td>\n",
       "      <td>0.154266</td>\n",
       "      <td>0.695818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.676254</td>\n",
       "      <td>0.596377</td>\n",
       "      <td>0.114229</td>\n",
       "      <td>0.834915</td>\n",
       "      <td>0.309675</td>\n",
       "      <td>0.632261</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>488</td>\n",
       "      <td>-2.331237</td>\n",
       "      <td>-2.574044</td>\n",
       "      <td>0.959037</td>\n",
       "      <td>2.024008</td>\n",
       "      <td>1.075365</td>\n",
       "      <td>-1.065263</td>\n",
       "      <td>0.287270</td>\n",
       "      <td>-0.091393</td>\n",
       "      <td>-0.223688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504612</td>\n",
       "      <td>0.227457</td>\n",
       "      <td>1.169242</td>\n",
       "      <td>-0.188325</td>\n",
       "      <td>0.270081</td>\n",
       "      <td>-0.105658</td>\n",
       "      <td>-0.214588</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>441.609384</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>384</td>\n",
       "      <td>0.450790</td>\n",
       "      <td>-2.168533</td>\n",
       "      <td>-0.824039</td>\n",
       "      <td>-0.351021</td>\n",
       "      <td>-0.984033</td>\n",
       "      <td>-0.079024</td>\n",
       "      <td>0.274409</td>\n",
       "      <td>-0.183159</td>\n",
       "      <td>-0.949651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157792</td>\n",
       "      <td>-1.057971</td>\n",
       "      <td>-0.559170</td>\n",
       "      <td>-0.469404</td>\n",
       "      <td>0.310075</td>\n",
       "      <td>1.076463</td>\n",
       "      <td>-0.173558</td>\n",
       "      <td>0.065210</td>\n",
       "      <td>510.220000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>478</td>\n",
       "      <td>-2.908935</td>\n",
       "      <td>-2.826193</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>1.984838</td>\n",
       "      <td>1.314504</td>\n",
       "      <td>-0.985084</td>\n",
       "      <td>0.319747</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.240584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574922</td>\n",
       "      <td>0.370750</td>\n",
       "      <td>1.207023</td>\n",
       "      <td>-0.287993</td>\n",
       "      <td>0.216378</td>\n",
       "      <td>-0.105579</td>\n",
       "      <td>-0.262300</td>\n",
       "      <td>-0.007489</td>\n",
       "      <td>474.537114</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>344</td>\n",
       "      <td>-3.495984</td>\n",
       "      <td>-4.088420</td>\n",
       "      <td>2.024845</td>\n",
       "      <td>-0.740363</td>\n",
       "      <td>1.128135</td>\n",
       "      <td>-1.231702</td>\n",
       "      <td>-0.086554</td>\n",
       "      <td>0.157807</td>\n",
       "      <td>1.677621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361562</td>\n",
       "      <td>-0.173006</td>\n",
       "      <td>1.280446</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.760879</td>\n",
       "      <td>-0.828147</td>\n",
       "      <td>-0.298700</td>\n",
       "      <td>-0.061615</td>\n",
       "      <td>456.710000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>473</td>\n",
       "      <td>-3.010884</td>\n",
       "      <td>-3.136585</td>\n",
       "      <td>1.131630</td>\n",
       "      <td>2.252680</td>\n",
       "      <td>1.394316</td>\n",
       "      <td>-1.012875</td>\n",
       "      <td>0.249309</td>\n",
       "      <td>-0.040987</td>\n",
       "      <td>-0.232448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654205</td>\n",
       "      <td>0.459271</td>\n",
       "      <td>1.342209</td>\n",
       "      <td>-0.311441</td>\n",
       "      <td>0.264170</td>\n",
       "      <td>-0.113851</td>\n",
       "      <td>-0.248761</td>\n",
       "      <td>0.029288</td>\n",
       "      <td>512.483598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>211</td>\n",
       "      <td>0.263523</td>\n",
       "      <td>-1.812897</td>\n",
       "      <td>-0.311087</td>\n",
       "      <td>0.412930</td>\n",
       "      <td>-0.794605</td>\n",
       "      <td>0.196365</td>\n",
       "      <td>0.416626</td>\n",
       "      <td>-0.062991</td>\n",
       "      <td>0.345392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>-1.042342</td>\n",
       "      <td>-0.480538</td>\n",
       "      <td>-0.414743</td>\n",
       "      <td>0.111706</td>\n",
       "      <td>0.819590</td>\n",
       "      <td>-0.168906</td>\n",
       "      <td>0.078203</td>\n",
       "      <td>526.960000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>152</td>\n",
       "      <td>0.095490</td>\n",
       "      <td>-2.557694</td>\n",
       "      <td>-1.134055</td>\n",
       "      <td>-0.435770</td>\n",
       "      <td>0.728493</td>\n",
       "      <td>4.090476</td>\n",
       "      <td>-0.429329</td>\n",
       "      <td>0.896879</td>\n",
       "      <td>1.004244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080098</td>\n",
       "      <td>-1.038538</td>\n",
       "      <td>-0.489645</td>\n",
       "      <td>1.108440</td>\n",
       "      <td>0.072603</td>\n",
       "      <td>0.893183</td>\n",
       "      <td>-0.146017</td>\n",
       "      <td>0.113110</td>\n",
       "      <td>614.870000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>486</td>\n",
       "      <td>-2.752793</td>\n",
       "      <td>-2.442101</td>\n",
       "      <td>1.110430</td>\n",
       "      <td>1.632425</td>\n",
       "      <td>1.261954</td>\n",
       "      <td>-0.892589</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>-0.021356</td>\n",
       "      <td>-0.205356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474265</td>\n",
       "      <td>0.295666</td>\n",
       "      <td>1.011050</td>\n",
       "      <td>-0.281253</td>\n",
       "      <td>0.142811</td>\n",
       "      <td>-0.059430</td>\n",
       "      <td>-0.273350</td>\n",
       "      <td>-0.057663</td>\n",
       "      <td>411.360357</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>481</td>\n",
       "      <td>-2.864189</td>\n",
       "      <td>-3.043499</td>\n",
       "      <td>1.325544</td>\n",
       "      <td>2.091130</td>\n",
       "      <td>1.549342</td>\n",
       "      <td>-0.779524</td>\n",
       "      <td>-0.093282</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>-0.059481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620555</td>\n",
       "      <td>0.566154</td>\n",
       "      <td>1.190569</td>\n",
       "      <td>-0.390669</td>\n",
       "      <td>0.193969</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>-0.230736</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>438.290346</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>476</td>\n",
       "      <td>-2.965549</td>\n",
       "      <td>-2.965457</td>\n",
       "      <td>1.094355</td>\n",
       "      <td>2.112617</td>\n",
       "      <td>1.333557</td>\n",
       "      <td>-1.018622</td>\n",
       "      <td>0.322198</td>\n",
       "      <td>-0.055337</td>\n",
       "      <td>-0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611419</td>\n",
       "      <td>0.397974</td>\n",
       "      <td>1.278079</td>\n",
       "      <td>-0.290437</td>\n",
       "      <td>0.243052</td>\n",
       "      <td>-0.122311</td>\n",
       "      <td>-0.258293</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>497.443866</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>492</td>\n",
       "      <td>-0.789890</td>\n",
       "      <td>-1.379371</td>\n",
       "      <td>0.171334</td>\n",
       "      <td>-1.636756</td>\n",
       "      <td>-2.807266</td>\n",
       "      <td>0.726236</td>\n",
       "      <td>2.737602</td>\n",
       "      <td>-0.933999</td>\n",
       "      <td>-2.413730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256231</td>\n",
       "      <td>0.167077</td>\n",
       "      <td>0.540876</td>\n",
       "      <td>0.067496</td>\n",
       "      <td>0.479636</td>\n",
       "      <td>-0.098230</td>\n",
       "      <td>0.053527</td>\n",
       "      <td>-0.408050</td>\n",
       "      <td>632.400000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>549</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>-1.818313</td>\n",
       "      <td>1.077334</td>\n",
       "      <td>3.350537</td>\n",
       "      <td>-1.292195</td>\n",
       "      <td>1.546080</td>\n",
       "      <td>-0.282520</td>\n",
       "      <td>0.402055</td>\n",
       "      <td>0.928263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108768</td>\n",
       "      <td>-0.451008</td>\n",
       "      <td>-0.475113</td>\n",
       "      <td>-0.261082</td>\n",
       "      <td>0.181753</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>-0.019035</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>530.850000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>386</td>\n",
       "      <td>0.071936</td>\n",
       "      <td>-1.601999</td>\n",
       "      <td>-0.606193</td>\n",
       "      <td>1.591623</td>\n",
       "      <td>-0.364290</td>\n",
       "      <td>0.319968</td>\n",
       "      <td>0.856676</td>\n",
       "      <td>-0.052634</td>\n",
       "      <td>-0.188426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364078</td>\n",
       "      <td>-0.110442</td>\n",
       "      <td>-0.669058</td>\n",
       "      <td>-0.262152</td>\n",
       "      <td>0.555656</td>\n",
       "      <td>-0.288130</td>\n",
       "      <td>-0.089500</td>\n",
       "      <td>0.095584</td>\n",
       "      <td>575.330000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>486</td>\n",
       "      <td>-2.447909</td>\n",
       "      <td>-2.669579</td>\n",
       "      <td>0.980236</td>\n",
       "      <td>2.067354</td>\n",
       "      <td>1.121955</td>\n",
       "      <td>-1.065191</td>\n",
       "      <td>0.293544</td>\n",
       "      <td>-0.087528</td>\n",
       "      <td>-0.231430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530342</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>1.203103</td>\n",
       "      <td>-0.205602</td>\n",
       "      <td>0.271672</td>\n",
       "      <td>-0.112161</td>\n",
       "      <td>-0.220842</td>\n",
       "      <td>0.035097</td>\n",
       "      <td>455.923503</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>529</td>\n",
       "      <td>-1.693201</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>1.168646</td>\n",
       "      <td>-0.637328</td>\n",
       "      <td>0.918177</td>\n",
       "      <td>-0.278183</td>\n",
       "      <td>0.269765</td>\n",
       "      <td>0.140603</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178274</td>\n",
       "      <td>-0.196792</td>\n",
       "      <td>-0.251539</td>\n",
       "      <td>-0.265688</td>\n",
       "      <td>-0.361437</td>\n",
       "      <td>0.236193</td>\n",
       "      <td>-0.330682</td>\n",
       "      <td>-0.368193</td>\n",
       "      <td>0.999750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>312</td>\n",
       "      <td>-0.646360</td>\n",
       "      <td>0.353084</td>\n",
       "      <td>0.743939</td>\n",
       "      <td>-0.191564</td>\n",
       "      <td>0.861749</td>\n",
       "      <td>0.035664</td>\n",
       "      <td>0.285499</td>\n",
       "      <td>0.139948</td>\n",
       "      <td>-0.064847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148934</td>\n",
       "      <td>-0.306495</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>-0.916464</td>\n",
       "      <td>-0.970357</td>\n",
       "      <td>0.140924</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>-0.013638</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>499</td>\n",
       "      <td>-1.295440</td>\n",
       "      <td>-0.592966</td>\n",
       "      <td>1.989811</td>\n",
       "      <td>0.510601</td>\n",
       "      <td>1.478677</td>\n",
       "      <td>-0.388836</td>\n",
       "      <td>-0.144863</td>\n",
       "      <td>0.167076</td>\n",
       "      <td>0.234979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188326</td>\n",
       "      <td>0.594452</td>\n",
       "      <td>-0.067589</td>\n",
       "      <td>-0.040201</td>\n",
       "      <td>0.182026</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.111358</td>\n",
       "      <td>-0.141280</td>\n",
       "      <td>1.171263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>150</td>\n",
       "      <td>0.420672</td>\n",
       "      <td>0.492041</td>\n",
       "      <td>0.407986</td>\n",
       "      <td>0.284861</td>\n",
       "      <td>0.471951</td>\n",
       "      <td>-0.136929</td>\n",
       "      <td>0.234743</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>-0.076560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175434</td>\n",
       "      <td>-0.514391</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>-0.863402</td>\n",
       "      <td>-0.917422</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>1.489568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>480</td>\n",
       "      <td>1.039079</td>\n",
       "      <td>1.158034</td>\n",
       "      <td>-1.555174</td>\n",
       "      <td>1.500270</td>\n",
       "      <td>1.556148</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>0.499065</td>\n",
       "      <td>0.148227</td>\n",
       "      <td>-0.603040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060672</td>\n",
       "      <td>0.113742</td>\n",
       "      <td>-0.049555</td>\n",
       "      <td>-0.940148</td>\n",
       "      <td>0.525324</td>\n",
       "      <td>-0.188411</td>\n",
       "      <td>0.116878</td>\n",
       "      <td>0.087125</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>188</td>\n",
       "      <td>1.168433</td>\n",
       "      <td>0.319978</td>\n",
       "      <td>0.589693</td>\n",
       "      <td>1.044774</td>\n",
       "      <td>-0.376942</td>\n",
       "      <td>-0.677335</td>\n",
       "      <td>0.054472</td>\n",
       "      <td>-0.087670</td>\n",
       "      <td>-0.304110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.066970</td>\n",
       "      <td>-0.017274</td>\n",
       "      <td>0.532429</td>\n",
       "      <td>0.502070</td>\n",
       "      <td>-0.470804</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>8.090000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>553</td>\n",
       "      <td>-0.457560</td>\n",
       "      <td>0.186130</td>\n",
       "      <td>0.810769</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>-0.623424</td>\n",
       "      <td>0.194345</td>\n",
       "      <td>-0.009404</td>\n",
       "      <td>0.019612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225424</td>\n",
       "      <td>-0.460793</td>\n",
       "      <td>-0.095337</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>0.177932</td>\n",
       "      <td>-0.207061</td>\n",
       "      <td>-0.206347</td>\n",
       "      <td>1.123985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>549</td>\n",
       "      <td>-0.509941</td>\n",
       "      <td>-1.186967</td>\n",
       "      <td>1.478872</td>\n",
       "      <td>0.934451</td>\n",
       "      <td>1.171985</td>\n",
       "      <td>-0.165965</td>\n",
       "      <td>-1.101672</td>\n",
       "      <td>0.331478</td>\n",
       "      <td>0.526779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097952</td>\n",
       "      <td>0.273075</td>\n",
       "      <td>0.219984</td>\n",
       "      <td>-0.308013</td>\n",
       "      <td>-0.017825</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>-0.077760</td>\n",
       "      <td>-0.078646</td>\n",
       "      <td>1.403928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>477</td>\n",
       "      <td>-0.347240</td>\n",
       "      <td>1.140218</td>\n",
       "      <td>1.289066</td>\n",
       "      <td>0.061070</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>-0.960933</td>\n",
       "      <td>0.756415</td>\n",
       "      <td>-0.091273</td>\n",
       "      <td>-0.439727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257643</td>\n",
       "      <td>-0.667492</td>\n",
       "      <td>-0.005055</td>\n",
       "      <td>0.335569</td>\n",
       "      <td>-0.148201</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.245460</td>\n",
       "      <td>0.099186</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>97</td>\n",
       "      <td>1.273818</td>\n",
       "      <td>0.058009</td>\n",
       "      <td>-1.312428</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>2.244373</td>\n",
       "      <td>3.296211</td>\n",
       "      <td>-0.342625</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>-0.154915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017292</td>\n",
       "      <td>-0.182883</td>\n",
       "      <td>-0.146981</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.842883</td>\n",
       "      <td>-0.314281</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>269</td>\n",
       "      <td>1.320626</td>\n",
       "      <td>-1.174338</td>\n",
       "      <td>0.713431</td>\n",
       "      <td>-2.177941</td>\n",
       "      <td>-1.511234</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>-1.308064</td>\n",
       "      <td>0.298796</td>\n",
       "      <td>0.788814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354221</td>\n",
       "      <td>-0.365280</td>\n",
       "      <td>0.064822</td>\n",
       "      <td>-0.322003</td>\n",
       "      <td>0.152701</td>\n",
       "      <td>-0.003131</td>\n",
       "      <td>0.075103</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>406</td>\n",
       "      <td>-0.814054</td>\n",
       "      <td>1.538222</td>\n",
       "      <td>1.115690</td>\n",
       "      <td>-0.051667</td>\n",
       "      <td>0.092334</td>\n",
       "      <td>-1.013398</td>\n",
       "      <td>0.748851</td>\n",
       "      <td>-0.124814</td>\n",
       "      <td>-0.207407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311452</td>\n",
       "      <td>-0.627544</td>\n",
       "      <td>-0.016469</td>\n",
       "      <td>0.363403</td>\n",
       "      <td>-0.014631</td>\n",
       "      <td>0.076914</td>\n",
       "      <td>0.467478</td>\n",
       "      <td>0.228123</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>376</td>\n",
       "      <td>1.220896</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.545205</td>\n",
       "      <td>0.838269</td>\n",
       "      <td>-0.491896</td>\n",
       "      <td>-0.691202</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>0.348690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219679</td>\n",
       "      <td>-0.426644</td>\n",
       "      <td>-0.001156</td>\n",
       "      <td>0.451173</td>\n",
       "      <td>0.477482</td>\n",
       "      <td>0.265557</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>12.890000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>414</td>\n",
       "      <td>-1.043068</td>\n",
       "      <td>1.044582</td>\n",
       "      <td>0.292186</td>\n",
       "      <td>-2.378536</td>\n",
       "      <td>-0.189793</td>\n",
       "      <td>-0.920526</td>\n",
       "      <td>0.178722</td>\n",
       "      <td>-0.384095</td>\n",
       "      <td>0.237234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934064</td>\n",
       "      <td>0.316389</td>\n",
       "      <td>-0.004709</td>\n",
       "      <td>0.268738</td>\n",
       "      <td>-0.177186</td>\n",
       "      <td>-0.126936</td>\n",
       "      <td>0.290259</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>46</td>\n",
       "      <td>-0.378245</td>\n",
       "      <td>0.732925</td>\n",
       "      <td>-0.120154</td>\n",
       "      <td>0.185755</td>\n",
       "      <td>2.594269</td>\n",
       "      <td>3.797183</td>\n",
       "      <td>0.059088</td>\n",
       "      <td>0.976768</td>\n",
       "      <td>-0.412661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107582</td>\n",
       "      <td>-0.157140</td>\n",
       "      <td>-0.194659</td>\n",
       "      <td>1.013897</td>\n",
       "      <td>0.145503</td>\n",
       "      <td>-0.237620</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>0.202788</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>273</td>\n",
       "      <td>-1.009456</td>\n",
       "      <td>1.186971</td>\n",
       "      <td>-0.483997</td>\n",
       "      <td>1.877550</td>\n",
       "      <td>0.212349</td>\n",
       "      <td>-0.510227</td>\n",
       "      <td>-0.989987</td>\n",
       "      <td>0.707891</td>\n",
       "      <td>-1.324622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164477</td>\n",
       "      <td>-0.224472</td>\n",
       "      <td>-0.128711</td>\n",
       "      <td>-0.599473</td>\n",
       "      <td>-0.738325</td>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.245093</td>\n",
       "      <td>0.060652</td>\n",
       "      <td>0.540609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>328</td>\n",
       "      <td>-4.236419</td>\n",
       "      <td>-4.459784</td>\n",
       "      <td>1.381813</td>\n",
       "      <td>1.117080</td>\n",
       "      <td>6.044486</td>\n",
       "      <td>-3.498447</td>\n",
       "      <td>-2.740892</td>\n",
       "      <td>0.372155</td>\n",
       "      <td>-0.214338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397072</td>\n",
       "      <td>-0.432517</td>\n",
       "      <td>0.612507</td>\n",
       "      <td>-1.016362</td>\n",
       "      <td>0.630373</td>\n",
       "      <td>-0.498141</td>\n",
       "      <td>-0.094774</td>\n",
       "      <td>0.208038</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>545</td>\n",
       "      <td>-1.030747</td>\n",
       "      <td>0.894747</td>\n",
       "      <td>2.375620</td>\n",
       "      <td>0.218372</td>\n",
       "      <td>-0.862126</td>\n",
       "      <td>-0.180483</td>\n",
       "      <td>-0.260299</td>\n",
       "      <td>0.592381</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.020326</td>\n",
       "      <td>-0.216072</td>\n",
       "      <td>0.435670</td>\n",
       "      <td>0.192864</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>-0.066266</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>115</td>\n",
       "      <td>1.253704</td>\n",
       "      <td>0.348673</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.688416</td>\n",
       "      <td>-0.363193</td>\n",
       "      <td>-1.051306</td>\n",
       "      <td>0.083605</td>\n",
       "      <td>-0.197312</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286406</td>\n",
       "      <td>-0.828960</td>\n",
       "      <td>0.127569</td>\n",
       "      <td>0.326397</td>\n",
       "      <td>0.215008</td>\n",
       "      <td>0.095302</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>347</td>\n",
       "      <td>-0.812644</td>\n",
       "      <td>0.307358</td>\n",
       "      <td>0.811401</td>\n",
       "      <td>-0.262371</td>\n",
       "      <td>0.870712</td>\n",
       "      <td>-0.014188</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.140052</td>\n",
       "      <td>-0.051402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153594</td>\n",
       "      <td>-0.289069</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.813092</td>\n",
       "      <td>-0.873634</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>-0.050355</td>\n",
       "      <td>-0.069957</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>135</td>\n",
       "      <td>1.101762</td>\n",
       "      <td>-0.119018</td>\n",
       "      <td>1.139150</td>\n",
       "      <td>1.844961</td>\n",
       "      <td>-0.817909</td>\n",
       "      <td>0.247770</td>\n",
       "      <td>-0.684181</td>\n",
       "      <td>0.283096</td>\n",
       "      <td>0.286763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120201</td>\n",
       "      <td>0.371305</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>0.113901</td>\n",
       "      <td>0.152014</td>\n",
       "      <td>1.126049</td>\n",
       "      <td>-0.038695</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>73</td>\n",
       "      <td>1.162281</td>\n",
       "      <td>1.248178</td>\n",
       "      <td>-1.581317</td>\n",
       "      <td>1.475024</td>\n",
       "      <td>1.138357</td>\n",
       "      <td>-1.020373</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>-0.136762</td>\n",
       "      <td>-0.805505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124012</td>\n",
       "      <td>-0.227150</td>\n",
       "      <td>-0.199185</td>\n",
       "      <td>-0.289757</td>\n",
       "      <td>0.776244</td>\n",
       "      <td>-0.283950</td>\n",
       "      <td>0.056747</td>\n",
       "      <td>0.084706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>519</td>\n",
       "      <td>0.764614</td>\n",
       "      <td>1.706191</td>\n",
       "      <td>-1.755823</td>\n",
       "      <td>1.557386</td>\n",
       "      <td>1.101083</td>\n",
       "      <td>-1.529455</td>\n",
       "      <td>0.917702</td>\n",
       "      <td>-0.190132</td>\n",
       "      <td>-0.748935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085310</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>-0.459204</td>\n",
       "      <td>-0.421570</td>\n",
       "      <td>0.260012</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>365</td>\n",
       "      <td>-0.456586</td>\n",
       "      <td>0.541708</td>\n",
       "      <td>1.331297</td>\n",
       "      <td>0.559447</td>\n",
       "      <td>0.184045</td>\n",
       "      <td>0.612792</td>\n",
       "      <td>0.632759</td>\n",
       "      <td>0.152253</td>\n",
       "      <td>-0.202196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134465</td>\n",
       "      <td>-0.316126</td>\n",
       "      <td>-0.072149</td>\n",
       "      <td>-0.749725</td>\n",
       "      <td>-0.039644</td>\n",
       "      <td>0.472098</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.099985</td>\n",
       "      <td>74.220000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>60</td>\n",
       "      <td>1.069141</td>\n",
       "      <td>0.043786</td>\n",
       "      <td>0.309867</td>\n",
       "      <td>1.316694</td>\n",
       "      <td>-0.462394</td>\n",
       "      <td>-0.857888</td>\n",
       "      <td>0.207904</td>\n",
       "      <td>-0.191270</td>\n",
       "      <td>0.056398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115775</td>\n",
       "      <td>0.164781</td>\n",
       "      <td>-0.177564</td>\n",
       "      <td>0.391891</td>\n",
       "      <td>0.624682</td>\n",
       "      <td>-0.288376</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>524</td>\n",
       "      <td>-0.795322</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>2.210568</td>\n",
       "      <td>0.268145</td>\n",
       "      <td>-0.506875</td>\n",
       "      <td>-0.303840</td>\n",
       "      <td>-0.180829</td>\n",
       "      <td>0.085540</td>\n",
       "      <td>0.248346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210198</td>\n",
       "      <td>0.869922</td>\n",
       "      <td>-0.118656</td>\n",
       "      <td>0.744594</td>\n",
       "      <td>-0.504294</td>\n",
       "      <td>1.259841</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>0.128346</td>\n",
       "      <td>37.890000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>77</td>\n",
       "      <td>-1.185954</td>\n",
       "      <td>-0.218808</td>\n",
       "      <td>1.101005</td>\n",
       "      <td>-1.406997</td>\n",
       "      <td>1.261783</td>\n",
       "      <td>-0.337709</td>\n",
       "      <td>1.191747</td>\n",
       "      <td>-0.427383</td>\n",
       "      <td>0.276679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362830</td>\n",
       "      <td>-0.418817</td>\n",
       "      <td>0.521401</td>\n",
       "      <td>-0.922427</td>\n",
       "      <td>-0.139840</td>\n",
       "      <td>0.709787</td>\n",
       "      <td>-0.076249</td>\n",
       "      <td>-0.383942</td>\n",
       "      <td>49.950000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>53</td>\n",
       "      <td>-1.198968</td>\n",
       "      <td>-1.474100</td>\n",
       "      <td>1.840326</td>\n",
       "      <td>-4.515824</td>\n",
       "      <td>0.327567</td>\n",
       "      <td>-0.174469</td>\n",
       "      <td>0.959726</td>\n",
       "      <td>-1.026456</td>\n",
       "      <td>1.700435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453169</td>\n",
       "      <td>0.334614</td>\n",
       "      <td>-0.364541</td>\n",
       "      <td>-0.310186</td>\n",
       "      <td>-0.302599</td>\n",
       "      <td>-1.243924</td>\n",
       "      <td>-1.123457</td>\n",
       "      <td>-0.734351</td>\n",
       "      <td>89.170000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time        V1        V2        V3        V4        V5        V6  \\\n",
       "877    487 -2.747415 -2.428872  1.110836  1.620286  1.260144 -0.889403   \n",
       "535    403  0.699599 -2.631727  0.661576 -0.707254 -2.261000  0.360969   \n",
       "478    353  0.150999 -3.002120  0.824301  0.231721 -2.621415  0.128843   \n",
       "593    444  0.548588 -1.106394 -0.650881  0.572799 -0.148004  0.055981   \n",
       "990    472 -3.000916 -3.085668  1.101613  2.246211  1.351878 -1.061522   \n",
       "433    312  0.230981 -2.000483  0.555155  0.600646 -1.551024  0.242333   \n",
       "851    482 -2.843333 -2.664819  1.103589  1.836774  1.292425 -0.946223   \n",
       "1328   476 -2.954564 -2.938436  1.095185  2.087824  1.329860 -1.012114   \n",
       "235    156 -3.494861 -2.894450  1.637989 -0.274976 -0.389203 -0.703275   \n",
       "1435   488 -2.331237 -2.574044  0.959037  2.024008  1.075365 -1.065263   \n",
       "623    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "519    384  0.450790 -2.168533 -0.824039 -0.351021 -0.984033 -0.079024   \n",
       "1266   478 -2.908935 -2.826193  1.098633  1.984838  1.314504 -0.985084   \n",
       "468    344 -3.495984 -4.088420  2.024845 -0.740363  1.128135 -1.231702   \n",
       "1496   473 -3.010884 -3.136585  1.131630  2.252680  1.394316 -1.012875   \n",
       "296    211  0.263523 -1.812897 -0.311087  0.412930 -0.794605  0.196365   \n",
       "227    152  0.095490 -2.557694 -1.134055 -0.435770  0.728493  4.090476   \n",
       "1134   486 -2.752793 -2.442101  1.110430  1.632425  1.261954 -0.892589   \n",
       "861    481 -2.864189 -3.043499  1.325544  2.091130  1.549342 -0.779524   \n",
       "1479   476 -2.965549 -2.965457  1.094355  2.112617  1.333557 -1.018622   \n",
       "651    492 -0.789890 -1.379371  0.171334 -1.636756 -2.807266  0.726236   \n",
       "727    549  0.033854 -1.818313  1.077334  3.350537 -1.292195  1.546080   \n",
       "521    386  0.071936 -1.601999 -0.606193  1.591623 -0.364290  0.319968   \n",
       "801    486 -2.447909 -2.669579  0.980236  2.067354  1.121955 -1.065191   \n",
       "874    529 -1.693201  0.065219  1.168646 -0.637328  0.918177 -0.278183   \n",
       "1009   312 -0.646360  0.353084  0.743939 -0.191564  0.861749  0.035664   \n",
       "1237   499 -1.295440 -0.592966  1.989811  0.510601  1.478677 -0.388836   \n",
       "1502   150  0.420672  0.492041  0.407986  0.284861  0.471951 -0.136929   \n",
       "631    480  1.039079  1.158034 -1.555174  1.500270  1.556148  0.036028   \n",
       "265    188  1.168433  0.319978  0.589693  1.044774 -0.376942 -0.677335   \n",
       "929    553 -0.457560  0.186130  0.810769 -0.080639  0.374057 -0.623424   \n",
       "1041   549 -0.509941 -1.186967  1.478872  0.934451  1.171985 -0.165965   \n",
       "626    477 -0.347240  1.140218  1.289066  0.061070  0.032514 -0.960933   \n",
       "155     97  1.273818  0.058009 -1.312428 -0.048969  2.244373  3.296211   \n",
       "368    269  1.320626 -1.174338  0.713431 -2.177941 -1.511234  0.059314   \n",
       "244    164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "540    406 -0.814054  1.538222  1.115690 -0.051667  0.092334 -1.013398   \n",
       "508    376  1.220896  0.116243  0.545205  0.838269 -0.491896 -0.691202   \n",
       "556    414 -1.043068  1.044582  0.292186 -2.378536 -0.189793 -0.920526   \n",
       "71      46 -0.378245  0.732925 -0.120154  0.185755  2.594269  3.797183   \n",
       "1160   273 -1.009456  1.186971 -0.483997  1.877550  0.212349 -0.510227   \n",
       "450    328 -4.236419 -4.459784  1.381813  1.117080  6.044486 -3.498447   \n",
       "721    545 -1.030747  0.894747  2.375620  0.218372 -0.862126 -0.180483   \n",
       "1360   115  1.253704  0.348673  0.299879  0.688416 -0.363193 -1.051306   \n",
       "813    347 -0.812644  0.307358  0.811401 -0.262371  0.870712 -0.014188   \n",
       "202    135  1.101762 -0.119018  1.139150  1.844961 -0.817909  0.247770   \n",
       "108     73  1.162281  1.248178 -1.581317  1.475024  1.138357 -1.020373   \n",
       "688    519  0.764614  1.706191 -1.755823  1.557386  1.101083 -1.529455   \n",
       "494    365 -0.456586  0.541708  1.331297  0.559447  0.184045  0.612792   \n",
       "91      60  1.069141  0.043786  0.309867  1.316694 -0.462394 -0.857888   \n",
       "693    524 -0.795322  0.081238  2.210568  0.268145 -0.506875 -0.303840   \n",
       "120     77 -1.185954 -0.218808  1.101005 -1.406997  1.261783 -0.337709   \n",
       "83      53 -1.198968 -1.474100  1.840326 -4.515824  0.327567 -0.174469   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "877   0.312755 -0.020497 -0.204143  ...  0.470798  0.293080  1.004300   \n",
       "535  -1.069656 -0.053544 -0.833647  ...  0.049882  0.068132 -0.499373   \n",
       "478  -0.755233  0.095956  0.638219  ...  0.521719  0.320297 -0.591253   \n",
       "593   0.551553 -0.133970 -0.204957  ...  0.212745 -0.279628 -0.568729   \n",
       "990   0.336726 -0.069583 -0.268357  ...  0.649700  0.432372  1.343044   \n",
       "433  -0.133183  0.105984  1.248546  ...  0.079669 -0.610378 -0.309129   \n",
       "851   0.316907 -0.035817 -0.225783  ...  0.532632  0.339204  1.124686   \n",
       "1328  0.321723 -0.053583 -0.250879  ...  0.604337  0.392691  1.264292   \n",
       "235   0.444194  0.154266  0.695818  ...  0.017010 -0.063521  0.676254   \n",
       "1435  0.287270 -0.091393 -0.223688  ...  0.504612  0.227457  1.169242   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "519   0.274409 -0.183159 -0.949651  ... -0.157792 -1.057971 -0.559170   \n",
       "1266  0.319747 -0.046295 -0.240584  ...  0.574922  0.370750  1.207023   \n",
       "468  -0.086554  0.157807  1.677621  ...  0.361562 -0.173006  1.280446   \n",
       "1496  0.249309 -0.040987 -0.232448  ...  0.654205  0.459271  1.342209   \n",
       "296   0.416626 -0.062991  0.345392  ...  0.002672 -1.042342 -0.480538   \n",
       "227  -0.429329  0.896879  1.004244  ...  0.080098 -1.038538 -0.489645   \n",
       "1134  0.312988 -0.021356 -0.205356  ...  0.474265  0.295666  1.011050   \n",
       "861  -0.093282  0.079430 -0.059481  ...  0.620555  0.566154  1.190569   \n",
       "1479  0.322198 -0.055337 -0.253357  ...  0.611419  0.397974  1.278079   \n",
       "651   2.737602 -0.933999 -2.413730  ... -0.256231  0.167077  0.540876   \n",
       "727  -0.282520  0.402055  0.928263  ...  0.108768 -0.451008 -0.475113   \n",
       "521   0.856676 -0.052634 -0.188426  ...  0.364078 -0.110442 -0.669058   \n",
       "801   0.293544 -0.087528 -0.231430  ...  0.530342  0.261529  1.203103   \n",
       "874   0.269765  0.140603  0.019795  ... -0.178274 -0.196792 -0.251539   \n",
       "1009  0.285499  0.139948 -0.064847  ... -0.148934 -0.306495 -0.013011   \n",
       "1237 -0.144863  0.167076  0.234979  ...  0.188326  0.594452 -0.067589   \n",
       "1502  0.234743  0.038898 -0.076560  ... -0.175434 -0.514391  0.144274   \n",
       "631   0.499065  0.148227 -0.603040  ... -0.060672  0.113742 -0.049555   \n",
       "265   0.054472 -0.087670 -0.304110  ...  0.020051  0.066970 -0.017274   \n",
       "929   0.194345 -0.009404  0.019612  ... -0.225424 -0.460793 -0.095337   \n",
       "1041 -1.101672  0.331478  0.526779  ...  0.097952  0.273075  0.219984   \n",
       "626   0.756415 -0.091273 -0.439727  ... -0.257643 -0.667492 -0.005055   \n",
       "155  -0.342625  0.760870 -0.154915  ... -0.017292 -0.182883 -0.146981   \n",
       "368  -1.308064  0.298796  0.788814  ... -0.354221 -0.365280  0.064822   \n",
       "244   0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "540   0.748851 -0.124814 -0.207407  ... -0.311452 -0.627544 -0.016469   \n",
       "508   0.037916 -0.180524  0.348690  ... -0.219679 -0.426644 -0.001156   \n",
       "556   0.178722 -0.384095  0.237234  ...  0.934064  0.316389 -0.004709   \n",
       "71    0.059088  0.976768 -0.412661  ... -0.107582 -0.157140 -0.194659   \n",
       "1160 -0.989987  0.707891 -1.324622  ...  0.164477 -0.224472 -0.128711   \n",
       "450  -2.740892  0.372155 -0.214338  ...  0.397072 -0.432517  0.612507   \n",
       "721  -0.260299  0.592381 -0.017765  ...  0.006203  0.020326 -0.216072   \n",
       "1360  0.083605 -0.197312  0.029580  ... -0.286406 -0.828960  0.127569   \n",
       "813   0.283000  0.140052 -0.051402  ... -0.153594 -0.289069 -0.050900   \n",
       "202  -0.684181  0.283096  0.286763  ...  0.120201  0.371305 -0.016047   \n",
       "108   0.638387 -0.136762 -0.805505  ... -0.124012 -0.227150 -0.199185   \n",
       "688   0.917702 -0.190132 -0.748935  ... -0.085310  0.022452  0.056012   \n",
       "494   0.632759  0.152253 -0.202196  ... -0.134465 -0.316126 -0.072149   \n",
       "91    0.207904 -0.191270  0.056398  ...  0.115775  0.164781 -0.177564   \n",
       "693  -0.180829  0.085540  0.248346  ...  0.210198  0.869922 -0.118656   \n",
       "120   1.191747 -0.427383  0.276679  ... -0.362830 -0.418817  0.521401   \n",
       "83    0.959726 -1.026456  1.700435  ... -0.453169  0.334614 -0.364541   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  clusters  \n",
       "877  -0.281021  0.140277 -0.057841 -0.273731 -0.059391  409.184350         5  \n",
       "535  -0.028743  0.460852 -0.003403  0.020449  0.092994  411.880000         5  \n",
       "478   0.447557  0.164080 -0.131897 -0.047547  0.130266  611.760000         5  \n",
       "593  -0.779207  0.507367  0.410569 -0.121734  0.056038  399.070000         5  \n",
       "990  -0.280118  0.282041 -0.151577 -0.249785  0.032510  518.361360         5  \n",
       "433   0.167024 -0.137728  0.922421 -0.113353  0.093969  493.440000         5  \n",
       "851  -0.285161  0.185469 -0.086190 -0.266942 -0.028569  447.993824         5  \n",
       "1328 -0.289963  0.237876 -0.119064 -0.259070  0.007173  492.999212         5  \n",
       "235   0.596377  0.114229  0.834915  0.309675  0.632261  500.000000         5  \n",
       "1435 -0.188325  0.270081 -0.105658 -0.214588  0.034966  441.609384         5  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.000000         5  \n",
       "519  -0.469404  0.310075  1.076463 -0.173558  0.065210  510.220000         5  \n",
       "1266 -0.287993  0.216378 -0.105579 -0.262300 -0.007489  474.537114         5  \n",
       "468   0.012697  0.760879 -0.828147 -0.298700 -0.061615  456.710000         5  \n",
       "1496 -0.311441  0.264170 -0.113851 -0.248761  0.029288  512.483598         5  \n",
       "296  -0.414743  0.111706  0.819590 -0.168906  0.078203  526.960000         5  \n",
       "227   1.108440  0.072603  0.893183 -0.146017  0.113110  614.870000         5  \n",
       "1134 -0.281253  0.142811 -0.059430 -0.273350 -0.057663  411.360357         5  \n",
       "861  -0.390669  0.193969  0.027697 -0.230736  0.000198  438.290346         5  \n",
       "1479 -0.290437  0.243052 -0.122311 -0.258293  0.010703  497.443866         5  \n",
       "651   0.067496  0.479636 -0.098230  0.053527 -0.408050  632.400000         5  \n",
       "727  -0.261082  0.181753  0.025919 -0.019035  0.117564  530.850000         5  \n",
       "521  -0.262152  0.555656 -0.288130 -0.089500  0.095584  575.330000         5  \n",
       "801  -0.205602  0.271672 -0.112161 -0.220842  0.035097  455.923503         5  \n",
       "874  -0.265688 -0.361437  0.236193 -0.330682 -0.368193    0.999750         0  \n",
       "1009 -0.916464 -0.970357  0.140924  0.002582 -0.013638    0.993973         0  \n",
       "1237 -0.040201  0.182026 -0.003475 -0.111358 -0.141280    1.171263         0  \n",
       "1502 -0.863402 -0.917422  0.081081  0.156784  0.171610    1.489568         0  \n",
       "631  -0.940148  0.525324 -0.188411  0.116878  0.087125    0.750000         0  \n",
       "265   0.532429  0.502070 -0.470804  0.021893  0.019337    8.090000         0  \n",
       "929   0.010752 -0.097262  0.177932 -0.207061 -0.206347    1.123985         0  \n",
       "1041 -0.308013 -0.017825  0.510296 -0.077760 -0.078646    1.403928         0  \n",
       "626   0.335569 -0.148201  0.071064  0.245460  0.099186    9.510000         0  \n",
       "155   1.004700  0.842883 -0.314281  0.013744  0.015907   17.800000         0  \n",
       "368  -0.322003  0.152701 -0.003131  0.075103  0.010941    2.600000         0  \n",
       "244  -1.363967 -1.389079  0.075412  0.231750  0.230171    0.990000         0  \n",
       "540   0.363403 -0.014631  0.076914  0.467478  0.228123    1.980000         0  \n",
       "508   0.451173  0.477482  0.265557 -0.018555  0.015656   12.890000         0  \n",
       "556   0.268738 -0.177186 -0.126936  0.290259  0.127530    1.000000         0  \n",
       "71    1.013897  0.145503 -0.237620  0.411372  0.202788   11.450000         0  \n",
       "1160 -0.599473 -0.738325  0.121907  0.245093  0.060652    0.540609         0  \n",
       "450  -1.016362  0.630373 -0.498141 -0.094774  0.208038    2.500000         0  \n",
       "721   0.435670  0.192864  0.351030 -0.066266  0.005583    3.980000         0  \n",
       "1360  0.326397  0.215008  0.095302 -0.023079  0.030582    2.690000         0  \n",
       "813  -0.813092 -0.873634  0.156057 -0.050355 -0.069957    0.994890         0  \n",
       "202   0.113901  0.152014  1.126049 -0.038695  0.012339   22.370000         0  \n",
       "108  -0.289757  0.776244 -0.283950  0.056747  0.084706    1.000000         0  \n",
       "688   0.008466 -0.459204 -0.421570  0.260012  0.006571    0.890000         0  \n",
       "494  -0.749725 -0.039644  0.472098  0.075612  0.099985   74.220000         7  \n",
       "91    0.391891  0.624682 -0.288376 -0.001312  0.035812   89.400000         7  \n",
       "693   0.744594 -0.504294  1.259841  0.014692  0.128346   37.890000         7  \n",
       "120  -0.922427 -0.139840  0.709787 -0.076249 -0.383942   49.950000         7  \n",
       "83   -0.310186 -0.302599 -1.243924 -1.123457 -0.734351   89.170000         7  \n",
       "\n",
       "[53 rows x 31 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    31\n",
       "1    22\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01}\n",
      "BernoulliNB accuracy: 63.00%\n",
      "{'max_depth': 50}\n",
      "DecisionTreeClassifier accuracy: 65.58%\n",
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier accuracy: 54.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "LogisticRegression accuracy: 68.23%\n",
      "{'max_depth': 3, 'n_estimators': 100}\n",
      "RandomForestClassifier accuracy: 73.32%\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "SVC accuracy: 63.68%\n"
     ]
    }
   ],
   "source": [
    "cluster_accuracy=[]\n",
    "for model, para in models:\n",
    "    grid = GridSearchCV(model, para, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    cluster_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>simple</th>\n",
       "      <th>system</th>\n",
       "      <th>stratified</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.630007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.959002</td>\n",
       "      <td>0.655804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.545146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.901070</td>\n",
       "      <td>0.682281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.988414</td>\n",
       "      <td>0.733198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.636796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models    simple    system  stratified   cluster\n",
       "0   Bernoulli  0.848725  0.773315    0.863636  0.630007\n",
       "1    Decision  0.940193  0.862491    0.959002  0.655804\n",
       "2  Kneighbors  0.803870  0.637849    0.808378  0.545146\n",
       "3    logistic  0.913808  0.843431    0.901070  0.682281\n",
       "4      Random  0.992084  0.915589    0.988414  0.733198\n",
       "5         SVC  0.871592  0.677332    0.903743  0.636796"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['cluster'] = cluster_accuracy\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new,y_new,test_size=0.3,random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5}\n",
      "BernoulliNB accuracy: 84.93%\n",
      "{'max_depth': 50}\n",
      "DecisionTreeClassifier accuracy: 98.03%\n",
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier accuracy: 84.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ekasp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100}\n",
      "LogisticRegression accuracy: 92.14%\n",
      "{'max_depth': 50, 'n_estimators': 200}\n",
      "RandomForestClassifier accuracy: 99.34%\n",
      "{'C': 100, 'gamma': 0.001}\n",
      "SVC accuracy: 92.58%\n"
     ]
    }
   ],
   "source": [
    "normal_accuracy=[]\n",
    "for model, para in models:\n",
    "    grid = GridSearchCV(model, para, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    accuracy = best_model.score(X_test, y_test)\n",
    "    normal_accuracy.append(accuracy)\n",
    "    print(type(model).__name__ + ' accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>simple</th>\n",
       "      <th>system</th>\n",
       "      <th>stratified</th>\n",
       "      <th>cluster</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.630007</td>\n",
       "      <td>0.849345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision</td>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.959002</td>\n",
       "      <td>0.655804</td>\n",
       "      <td>0.980349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kneighbors</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.545146</td>\n",
       "      <td>0.840611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.901070</td>\n",
       "      <td>0.682281</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.988414</td>\n",
       "      <td>0.733198</td>\n",
       "      <td>0.993450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.636796</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models    simple    system  stratified   cluster    normal\n",
       "0   Bernoulli  0.848725  0.773315    0.863636  0.630007  0.849345\n",
       "1    Decision  0.940193  0.862491    0.959002  0.655804  0.980349\n",
       "2  Kneighbors  0.803870  0.637849    0.808378  0.545146  0.840611\n",
       "3    logistic  0.913808  0.843431    0.901070  0.682281  0.921397\n",
       "4      Random  0.992084  0.915589    0.988414  0.733198  0.993450\n",
       "5         SVC  0.871592  0.677332    0.903743  0.636796  0.925764"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['normal'] = normal_accuracy\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simple</th>\n",
       "      <th>system</th>\n",
       "      <th>stratified</th>\n",
       "      <th>cluster</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bernoulli</th>\n",
       "      <td>0.848725</td>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.630007</td>\n",
       "      <td>0.849345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision</th>\n",
       "      <td>0.940193</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.959002</td>\n",
       "      <td>0.655804</td>\n",
       "      <td>0.980349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kneighbors</th>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.545146</td>\n",
       "      <td>0.840611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.913808</td>\n",
       "      <td>0.843431</td>\n",
       "      <td>0.901070</td>\n",
       "      <td>0.682281</td>\n",
       "      <td>0.921397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.915589</td>\n",
       "      <td>0.988414</td>\n",
       "      <td>0.733198</td>\n",
       "      <td>0.993450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.636796</td>\n",
       "      <td>0.925764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              simple    system  stratified   cluster    normal\n",
       "Models                                                        \n",
       "Bernoulli   0.848725  0.773315    0.863636  0.630007  0.849345\n",
       "Decision    0.940193  0.862491    0.959002  0.655804  0.980349\n",
       "Kneighbors  0.803870  0.637849    0.808378  0.545146  0.840611\n",
       "logistic    0.913808  0.843431    0.901070  0.682281  0.921397\n",
       "Random      0.992084  0.915589    0.988414  0.733198  0.993450\n",
       "SVC         0.871592  0.677332    0.903743  0.636796  0.925764"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = table.set_index('Models')\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max accuracy :\n",
    "\n",
    "\n",
    "The random forest model gives best accuracy for all models "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec044e8ff643357d7a8bcf5df711256ba0d7dda7c0007fb25b823abc5f121fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
